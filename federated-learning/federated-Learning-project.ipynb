{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from torchvision import datasets,transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0621 15:04:34.292070 140190934955776 secure_random.py:22] Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow (1.14.0). Fix this by compiling custom ops.\n",
      "W0621 15:04:34.309285 140190934955776 deprecation_wrapper.py:119] From /home/venktesh/anaconda3/envs/pysyft/lib/python3.6/site-packages/tf_encrypted/session.py:28: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import syft as sy\n",
    "import copy\n",
    "\n",
    "hook = sy.TorchHook(torch)\n",
    "from torch import nn,optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import helper\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize([0.5], [0.5])])\n",
    "# Download and load the training data\n",
    "trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "image, label = next(iter(trainloader))\n",
    "helper.imshow(image[0,:]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bob = sy.VirtualWorker(hook, id =\"bob\")\n",
    "alice = sy.VirtualWorker(hook, id=\"alice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import logging\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "def dataset_federate(dataset, workers):\n",
    "    logger.info(\"Scanning and sending data to {}...\".format(\", \".join([w.id for w in workers])))\n",
    "\n",
    "    data_size = math.ceil(len(dataset) / len(workers))\n",
    "\n",
    "    datasets = []\n",
    "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=data_size, drop_last=True)\n",
    "    for dataset_idx, (data, targets) in enumerate(data_loader):\n",
    "        worker = workers[dataset_idx % len(workers)]\n",
    "        logger.debug(\"Sending data to worker %s\", worker.id)\n",
    "        data = data.send(worker)\n",
    "        targets = targets.send(worker)\n",
    "        datasets.append(sy.BaseDataset(data, targets))  # .send(worker)\n",
    "\n",
    "    logger.debug(\"Done!\")\n",
    "    return sy.FederatedDataset(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "federated_train_loader = sy.FederatedDataLoader(dataset_federate(trainset,(bob, alice)),\n",
    "                                                batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#buildng the network\n",
    "\n",
    "from torch import nn \n",
    "model = nn.Sequential(\n",
    "                    nn.Linear(784,128),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(128,64),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(64,10),\n",
    "                    nn.LogSoftmax(dim=1)\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#definign criterion and optimizer\n",
    "\n",
    "from torch import  optim \n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 Loss: 2.334021\n",
      "Train Epoch: 0 Loss: 2.301412\n",
      "Train Epoch: 0 Loss: 2.297607\n",
      "Train Epoch: 0 Loss: 2.299713\n",
      "Train Epoch: 0 Loss: 2.296465\n",
      "Train Epoch: 0 Loss: 2.290740\n",
      "Train Epoch: 0 Loss: 2.277173\n",
      "Train Epoch: 0 Loss: 2.262392\n",
      "Train Epoch: 0 Loss: 2.275093\n",
      "Train Epoch: 0 Loss: 2.262860\n",
      "Train Epoch: 0 Loss: 2.265549\n",
      "Train Epoch: 0 Loss: 2.256385\n",
      "Train Epoch: 0 Loss: 2.253033\n",
      "Train Epoch: 0 Loss: 2.267847\n",
      "Train Epoch: 0 Loss: 2.247274\n",
      "Train Epoch: 0 Loss: 2.258145\n",
      "Train Epoch: 0 Loss: 2.213878\n",
      "Train Epoch: 0 Loss: 2.244611\n",
      "Train Epoch: 0 Loss: 2.234027\n",
      "Train Epoch: 0 Loss: 2.232936\n",
      "Train Epoch: 0 Loss: 2.241218\n",
      "Train Epoch: 0 Loss: 2.225300\n",
      "Train Epoch: 0 Loss: 2.224937\n",
      "Train Epoch: 0 Loss: 2.213945\n",
      "Train Epoch: 0 Loss: 2.227190\n",
      "Train Epoch: 0 Loss: 2.180004\n",
      "Train Epoch: 0 Loss: 2.168017\n",
      "Train Epoch: 0 Loss: 2.167094\n",
      "Train Epoch: 0 Loss: 2.171616\n",
      "Train Epoch: 0 Loss: 2.157797\n",
      "Train Epoch: 0 Loss: 2.170491\n",
      "Train Epoch: 0 Loss: 2.169739\n",
      "Epoch:  0\n",
      "Loss:  2.158106565475464\n",
      "Loss:  2.1542770862579346\n",
      "Loss:  2.1634137630462646\n",
      "Loss:  2.172687530517578\n",
      "Loss:  2.1788480281829834\n",
      "Loss:  2.1925604343414307\n",
      "Loss:  2.1436924934387207\n",
      "Loss:  2.1835739612579346\n",
      "Loss:  2.176518201828003\n",
      "Loss:  2.1623828411102295\n",
      "Loss:  2.173581600189209\n",
      "Loss:  2.147329092025757\n",
      "Loss:  2.181795120239258\n",
      "Loss:  2.1752097606658936\n",
      "Loss:  2.1909778118133545\n",
      "Loss:  2.131887435913086\n",
      "Loss:  2.1408369541168213\n",
      "Loss:  2.1431596279144287\n",
      "Loss:  2.180833578109741\n",
      "Loss:  2.166870355606079\n",
      "Loss:  2.182454824447632\n",
      "Loss:  2.1669414043426514\n",
      "Loss:  2.1456172466278076\n",
      "Loss:  2.1414568424224854\n",
      "Loss:  2.1702542304992676\n",
      "Loss:  2.1630706787109375\n",
      "Loss:  2.1876423358917236\n",
      "Loss:  2.1647307872772217\n",
      "Loss:  2.190704345703125\n",
      "Loss:  2.1620407104492188\n",
      "Loss:  2.1299407482147217\n",
      "Loss:  2.139165163040161\n",
      "Loss:  2.158956289291382\n",
      "Loss:  2.1775319576263428\n",
      "Loss:  2.16790771484375\n",
      "Loss:  2.167386293411255\n",
      "Loss:  2.151271104812622\n",
      "Loss:  2.2000033855438232\n",
      "Loss:  2.158961534500122\n",
      "Loss:  2.1665754318237305\n",
      "Loss:  2.1555542945861816\n",
      "Loss:  2.151609182357788\n",
      "Loss:  2.1628873348236084\n",
      "Loss:  2.1803395748138428\n",
      "Loss:  2.1501874923706055\n",
      "Loss:  2.1911442279815674\n",
      "Loss:  2.166229009628296\n",
      "Loss:  2.161362648010254\n",
      "Loss:  2.175360918045044\n",
      "Loss:  2.1427481174468994\n",
      "Loss:  2.163682699203491\n",
      "Loss:  2.151693820953369\n",
      "Loss:  2.1267120838165283\n",
      "Loss:  2.17016863822937\n",
      "Loss:  2.1324079036712646\n",
      "Loss:  2.15977144241333\n",
      "Loss:  2.1631312370300293\n",
      "Loss:  2.1677193641662598\n",
      "Loss:  2.161933183670044\n",
      "Loss:  2.1947600841522217\n",
      "Loss:  2.195655107498169\n",
      "Loss:  2.1543691158294678\n",
      "Loss:  2.156691789627075\n",
      "Loss:  2.1811861991882324\n",
      "Loss:  2.1700925827026367\n",
      "Loss:  2.1842808723449707\n",
      "Loss:  2.1911160945892334\n",
      "Loss:  2.1681840419769287\n",
      "Loss:  2.1302127838134766\n",
      "Loss:  2.165828227996826\n",
      "Loss:  2.197047472000122\n",
      "Loss:  2.146733522415161\n",
      "Loss:  2.1808888912200928\n",
      "Loss:  2.1820337772369385\n",
      "Loss:  2.1850156784057617\n",
      "Loss:  2.1279983520507812\n",
      "Loss:  2.1796483993530273\n",
      "Loss:  2.174569845199585\n",
      "Loss:  2.149635076522827\n",
      "Loss:  2.1827304363250732\n",
      "Loss:  2.1774513721466064\n",
      "Loss:  2.1620821952819824\n",
      "Loss:  2.1673285961151123\n",
      "Loss:  2.1744916439056396\n",
      "Loss:  2.1679201126098633\n",
      "Loss:  2.1742894649505615\n",
      "Loss:  2.18660831451416\n",
      "Loss:  2.1856775283813477\n",
      "Loss:  2.164177417755127\n",
      "Loss:  2.189619779586792\n",
      "Loss:  2.1615965366363525\n",
      "Loss:  2.153320789337158\n",
      "Loss:  2.1643433570861816\n",
      "Loss:  2.161696434020996\n",
      "Loss:  2.1583964824676514\n",
      "Loss:  2.1958682537078857\n",
      "Loss:  2.166745185852051\n",
      "Loss:  2.155102014541626\n",
      "Loss:  2.163821220397949\n",
      "Loss:  2.1543831825256348\n",
      "Loss:  2.139256000518799\n",
      "Loss:  2.164031505584717\n",
      "Loss:  2.1572370529174805\n",
      "Loss:  2.139798164367676\n",
      "Loss:  2.1806538105010986\n",
      "Loss:  2.1812729835510254\n",
      "Loss:  2.151475429534912\n",
      "Loss:  2.183730125427246\n",
      "Loss:  2.158740282058716\n",
      "Loss:  2.1648943424224854\n",
      "Loss:  2.1781184673309326\n",
      "Loss:  2.1521711349487305\n",
      "Loss:  2.191988945007324\n",
      "Loss:  2.1695141792297363\n",
      "Loss:  2.1758387088775635\n",
      "Loss:  2.1397063732147217\n",
      "Loss:  2.1501998901367188\n",
      "Loss:  2.177402973175049\n",
      "Loss:  2.1929705142974854\n",
      "Loss:  2.1696629524230957\n",
      "Loss:  2.150275945663452\n",
      "Loss:  2.1551709175109863\n",
      "Loss:  2.1685945987701416\n",
      "Loss:  2.153123617172241\n",
      "Loss:  2.1719048023223877\n",
      "Loss:  2.1926348209381104\n",
      "Loss:  2.1912333965301514\n",
      "Loss:  2.1648507118225098\n",
      "Loss:  2.1820061206817627\n",
      "Loss:  2.186669111251831\n",
      "Loss:  2.1512231826782227\n",
      "Loss:  2.139796257019043\n",
      "Loss:  2.173305034637451\n",
      "Loss:  2.1441023349761963\n",
      "Loss:  2.1632416248321533\n",
      "Loss:  2.172962188720703\n",
      "Loss:  2.204681396484375\n",
      "Loss:  2.1875662803649902\n",
      "Loss:  2.1448607444763184\n",
      "Loss:  2.1861846446990967\n",
      "Loss:  2.1765105724334717\n",
      "Loss:  2.195152997970581\n",
      "Loss:  2.1909570693969727\n",
      "Loss:  2.178287982940674\n",
      "Loss:  2.1683871746063232\n",
      "Loss:  2.1453118324279785\n",
      "Loss:  2.1793622970581055\n",
      "Loss:  2.14493465423584\n",
      "Loss:  2.145275115966797\n",
      "Loss:  2.1554930210113525\n",
      "Loss:  2.1533751487731934\n",
      "Loss:  2.1839513778686523\n",
      "Loss:  2.1471874713897705\n",
      "Loss:  2.1763453483581543\n",
      "Loss:  2.1909377574920654\n",
      "Loss:  2.1749107837677\n",
      "Loss:  2.1279308795928955\n",
      "Testing data accuracy: 44%\n",
      "Train Epoch: 1 Loss: 2.178601\n",
      "Train Epoch: 1 Loss: 2.141308\n",
      "Train Epoch: 1 Loss: 2.156807\n",
      "Train Epoch: 1 Loss: 2.159692\n",
      "Train Epoch: 1 Loss: 2.100597\n",
      "Train Epoch: 1 Loss: 2.116120\n",
      "Train Epoch: 1 Loss: 2.137967\n",
      "Train Epoch: 1 Loss: 2.119797\n",
      "Train Epoch: 1 Loss: 2.093824\n",
      "Train Epoch: 1 Loss: 2.093296\n",
      "Train Epoch: 1 Loss: 2.135413\n",
      "Train Epoch: 1 Loss: 2.106022\n",
      "Train Epoch: 1 Loss: 2.069972\n",
      "Train Epoch: 1 Loss: 2.092740\n",
      "Train Epoch: 1 Loss: 2.087246\n",
      "Train Epoch: 1 Loss: 2.084265\n",
      "Train Epoch: 1 Loss: 2.092671\n",
      "Train Epoch: 1 Loss: 2.071957\n",
      "Train Epoch: 1 Loss: 2.050629\n",
      "Train Epoch: 1 Loss: 2.043533\n",
      "Train Epoch: 1 Loss: 2.015568\n",
      "Train Epoch: 1 Loss: 2.000989\n",
      "Train Epoch: 1 Loss: 1.976712\n",
      "Train Epoch: 1 Loss: 1.962401\n",
      "Train Epoch: 1 Loss: 2.004752\n",
      "Train Epoch: 1 Loss: 2.002591\n",
      "Train Epoch: 1 Loss: 1.941272\n",
      "Train Epoch: 1 Loss: 1.989555\n",
      "Train Epoch: 1 Loss: 1.922365\n",
      "Train Epoch: 1 Loss: 1.928248\n",
      "Train Epoch: 1 Loss: 1.919880\n",
      "Train Epoch: 1 Loss: 1.963524\n",
      "Epoch:  1\n",
      "Loss:  1.8744348287582397\n",
      "Loss:  1.9108577966690063\n",
      "Loss:  1.894139289855957\n",
      "Loss:  1.9198251962661743\n",
      "Loss:  2.0210907459259033\n",
      "Loss:  1.9611599445343018\n",
      "Loss:  1.9491666555404663\n",
      "Loss:  1.9990545511245728\n",
      "Loss:  1.9492173194885254\n",
      "Loss:  1.924451470375061\n",
      "Loss:  1.9322816133499146\n",
      "Loss:  1.9693646430969238\n",
      "Loss:  1.9830822944641113\n",
      "Loss:  1.8986775875091553\n",
      "Loss:  1.9966248273849487\n",
      "Loss:  1.9144915342330933\n",
      "Loss:  1.9381656646728516\n",
      "Loss:  1.9857044219970703\n",
      "Loss:  1.9808491468429565\n",
      "Loss:  1.9303358793258667\n",
      "Loss:  1.9080145359039307\n",
      "Loss:  1.9339967966079712\n",
      "Loss:  1.9768691062927246\n",
      "Loss:  1.9282130002975464\n",
      "Loss:  2.0067479610443115\n",
      "Loss:  1.9668551683425903\n",
      "Loss:  1.9058603048324585\n",
      "Loss:  1.9105582237243652\n",
      "Loss:  1.9866070747375488\n",
      "Loss:  1.9486587047576904\n",
      "Loss:  1.9294131994247437\n",
      "Loss:  1.9018733501434326\n",
      "Loss:  1.90567946434021\n",
      "Loss:  1.9367601871490479\n",
      "Loss:  1.975139856338501\n",
      "Loss:  1.8727995157241821\n",
      "Loss:  2.0059025287628174\n",
      "Loss:  1.9581782817840576\n",
      "Loss:  1.9192650318145752\n",
      "Loss:  1.9166474342346191\n",
      "Loss:  1.8699191808700562\n",
      "Loss:  1.9240175485610962\n",
      "Loss:  1.926091194152832\n",
      "Loss:  1.946750521659851\n",
      "Loss:  1.9738575220108032\n",
      "Loss:  1.8950908184051514\n",
      "Loss:  1.9514214992523193\n",
      "Loss:  1.9333503246307373\n",
      "Loss:  1.9323316812515259\n",
      "Loss:  1.927410364151001\n",
      "Loss:  1.9186655282974243\n",
      "Loss:  1.9338194131851196\n",
      "Loss:  1.9537957906723022\n",
      "Loss:  1.9313082695007324\n",
      "Loss:  1.9704532623291016\n",
      "Loss:  1.9877171516418457\n",
      "Loss:  1.9668009281158447\n",
      "Loss:  1.8643816709518433\n",
      "Loss:  1.9914237260818481\n",
      "Loss:  1.9517884254455566\n",
      "Loss:  1.9315738677978516\n",
      "Loss:  1.930486798286438\n",
      "Loss:  1.9881333112716675\n",
      "Loss:  1.9285529851913452\n",
      "Loss:  1.9495482444763184\n",
      "Loss:  1.9220690727233887\n",
      "Loss:  1.912579894065857\n",
      "Loss:  2.0198023319244385\n",
      "Loss:  1.9449783563613892\n",
      "Loss:  1.9091404676437378\n",
      "Loss:  2.022242546081543\n",
      "Loss:  1.9705710411071777\n",
      "Loss:  1.920981526374817\n",
      "Loss:  1.9463285207748413\n",
      "Loss:  1.9291173219680786\n",
      "Loss:  1.8986586332321167\n",
      "Loss:  1.9443753957748413\n",
      "Loss:  1.9417617321014404\n",
      "Loss:  1.961775302886963\n",
      "Loss:  1.9462053775787354\n",
      "Loss:  1.880142331123352\n",
      "Loss:  1.9241909980773926\n",
      "Loss:  1.876893162727356\n",
      "Loss:  1.9248859882354736\n",
      "Loss:  1.9587212800979614\n",
      "Loss:  1.9273868799209595\n",
      "Loss:  1.9581326246261597\n",
      "Loss:  1.9536967277526855\n",
      "Loss:  1.9827203750610352\n",
      "Loss:  1.9047014713287354\n",
      "Loss:  1.9172992706298828\n",
      "Loss:  1.9643241167068481\n",
      "Loss:  1.885448932647705\n",
      "Loss:  1.9071755409240723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  1.9314230680465698\n",
      "Loss:  1.961931824684143\n",
      "Loss:  1.9365421533584595\n",
      "Loss:  1.9687604904174805\n",
      "Loss:  1.9801018238067627\n",
      "Loss:  1.978806972503662\n",
      "Loss:  1.9213593006134033\n",
      "Loss:  1.9091235399246216\n",
      "Loss:  1.8836129903793335\n",
      "Loss:  1.9327590465545654\n",
      "Loss:  1.952704668045044\n",
      "Loss:  1.9263843297958374\n",
      "Loss:  1.9227991104125977\n",
      "Loss:  1.9092568159103394\n",
      "Loss:  1.9472177028656006\n",
      "Loss:  1.940451741218567\n",
      "Loss:  1.9796644449234009\n",
      "Loss:  1.993282437324524\n",
      "Loss:  1.9470020532608032\n",
      "Loss:  1.91757333278656\n",
      "Loss:  1.9259041547775269\n",
      "Loss:  1.8927217721939087\n",
      "Loss:  1.8389661312103271\n",
      "Loss:  1.8476213216781616\n",
      "Loss:  1.9643290042877197\n",
      "Loss:  1.9313222169876099\n",
      "Loss:  1.967725157737732\n",
      "Loss:  1.9578741788864136\n",
      "Loss:  1.9647985696792603\n",
      "Loss:  1.8934926986694336\n",
      "Loss:  1.9714323282241821\n",
      "Loss:  1.954177737236023\n",
      "Loss:  1.9395772218704224\n",
      "Loss:  1.8839032649993896\n",
      "Loss:  1.9733200073242188\n",
      "Loss:  1.9092177152633667\n",
      "Loss:  1.8626341819763184\n",
      "Loss:  1.9174020290374756\n",
      "Loss:  1.9081155061721802\n",
      "Loss:  1.9315825700759888\n",
      "Loss:  1.8885337114334106\n",
      "Loss:  1.939892053604126\n",
      "Loss:  1.9522383213043213\n",
      "Loss:  1.8968713283538818\n",
      "Loss:  1.8859422206878662\n",
      "Loss:  1.94822359085083\n",
      "Loss:  1.962152123451233\n",
      "Loss:  1.9808037281036377\n",
      "Loss:  1.9303648471832275\n",
      "Loss:  1.9125028848648071\n",
      "Loss:  1.9167366027832031\n",
      "Loss:  1.910299301147461\n",
      "Loss:  1.9120490550994873\n",
      "Loss:  1.9309227466583252\n",
      "Loss:  1.9129403829574585\n",
      "Loss:  1.9094394445419312\n",
      "Loss:  1.9477540254592896\n",
      "Loss:  1.9353642463684082\n",
      "Loss:  1.9996333122253418\n",
      "Loss:  1.9563356637954712\n",
      "Loss:  1.9276690483093262\n",
      "Loss:  1.9391132593154907\n",
      "Loss:  1.7978078126907349\n",
      "Testing data accuracy: 48%\n",
      "Train Epoch: 2 Loss: 1.959512\n",
      "Train Epoch: 2 Loss: 1.926671\n",
      "Train Epoch: 2 Loss: 1.857775\n",
      "Train Epoch: 2 Loss: 1.878097\n",
      "Train Epoch: 2 Loss: 1.907391\n",
      "Train Epoch: 2 Loss: 1.907839\n",
      "Train Epoch: 2 Loss: 1.882021\n",
      "Train Epoch: 2 Loss: 1.876328\n",
      "Train Epoch: 2 Loss: 1.876928\n",
      "Train Epoch: 2 Loss: 1.937985\n",
      "Train Epoch: 2 Loss: 1.824672\n",
      "Train Epoch: 2 Loss: 1.821823\n",
      "Train Epoch: 2 Loss: 1.806238\n",
      "Train Epoch: 2 Loss: 1.842097\n",
      "Train Epoch: 2 Loss: 1.746743\n",
      "Train Epoch: 2 Loss: 1.800884\n",
      "Train Epoch: 2 Loss: 1.812853\n",
      "Train Epoch: 2 Loss: 1.817311\n",
      "Train Epoch: 2 Loss: 1.791944\n",
      "Train Epoch: 2 Loss: 1.672046\n",
      "Train Epoch: 2 Loss: 1.742381\n",
      "Train Epoch: 2 Loss: 1.702928\n",
      "Train Epoch: 2 Loss: 1.723119\n",
      "Train Epoch: 2 Loss: 1.704594\n",
      "Train Epoch: 2 Loss: 1.659568\n",
      "Train Epoch: 2 Loss: 1.715264\n",
      "Train Epoch: 2 Loss: 1.657993\n",
      "Train Epoch: 2 Loss: 1.752587\n",
      "Train Epoch: 2 Loss: 1.656402\n",
      "Train Epoch: 2 Loss: 1.638495\n",
      "Train Epoch: 2 Loss: 1.558442\n",
      "Train Epoch: 2 Loss: 1.628008\n",
      "Epoch:  2\n",
      "Loss:  1.6012977361679077\n",
      "Loss:  1.5549471378326416\n",
      "Loss:  1.6079063415527344\n",
      "Loss:  1.7312837839126587\n",
      "Loss:  1.620316743850708\n",
      "Loss:  1.5546408891677856\n",
      "Loss:  1.6815413236618042\n",
      "Loss:  1.6736098527908325\n",
      "Loss:  1.6505459547042847\n",
      "Loss:  1.5560414791107178\n",
      "Loss:  1.62525475025177\n",
      "Loss:  1.6894066333770752\n",
      "Loss:  1.5324828624725342\n",
      "Loss:  1.7436054944992065\n",
      "Loss:  1.5796782970428467\n",
      "Loss:  1.657926082611084\n",
      "Loss:  1.530548095703125\n",
      "Loss:  1.6645052433013916\n",
      "Loss:  1.5857160091400146\n",
      "Loss:  1.6146172285079956\n",
      "Loss:  1.5804202556610107\n",
      "Loss:  1.5553348064422607\n",
      "Loss:  1.5413715839385986\n",
      "Loss:  1.6517401933670044\n",
      "Loss:  1.5649148225784302\n",
      "Loss:  1.5491708517074585\n",
      "Loss:  1.7060493230819702\n",
      "Loss:  1.6289527416229248\n",
      "Loss:  1.6722347736358643\n",
      "Loss:  1.6545822620391846\n",
      "Loss:  1.5569524765014648\n",
      "Loss:  1.5936295986175537\n",
      "Loss:  1.5519839525222778\n",
      "Loss:  1.7220643758773804\n",
      "Loss:  1.606011152267456\n",
      "Loss:  1.550747036933899\n",
      "Loss:  1.566250205039978\n",
      "Loss:  1.6736314296722412\n",
      "Loss:  1.5864415168762207\n",
      "Loss:  1.6347196102142334\n",
      "Loss:  1.5630912780761719\n",
      "Loss:  1.5757527351379395\n",
      "Loss:  1.7074434757232666\n",
      "Loss:  1.676936149597168\n",
      "Loss:  1.568392276763916\n",
      "Loss:  1.6387428045272827\n",
      "Loss:  1.6063454151153564\n",
      "Loss:  1.639814019203186\n",
      "Loss:  1.5730397701263428\n",
      "Loss:  1.6156193017959595\n",
      "Loss:  1.646582841873169\n",
      "Loss:  1.5551267862319946\n",
      "Loss:  1.5502827167510986\n",
      "Loss:  1.607336163520813\n",
      "Loss:  1.6420035362243652\n",
      "Loss:  1.6421023607254028\n",
      "Loss:  1.5516051054000854\n",
      "Loss:  1.5898385047912598\n",
      "Loss:  1.6033143997192383\n",
      "Loss:  1.596359133720398\n",
      "Loss:  1.5709372758865356\n",
      "Loss:  1.5903440713882446\n",
      "Loss:  1.5061670541763306\n",
      "Loss:  1.573213815689087\n",
      "Loss:  1.6488484144210815\n",
      "Loss:  1.5963616371154785\n",
      "Loss:  1.617519497871399\n",
      "Loss:  1.5560826063156128\n",
      "Loss:  1.581595540046692\n",
      "Loss:  1.663375735282898\n",
      "Loss:  1.625468373298645\n",
      "Loss:  1.5688813924789429\n",
      "Loss:  1.5570913553237915\n",
      "Loss:  1.723288655281067\n",
      "Loss:  1.5113781690597534\n",
      "Loss:  1.5449631214141846\n",
      "Loss:  1.6861090660095215\n",
      "Loss:  1.6725656986236572\n",
      "Loss:  1.649118423461914\n",
      "Loss:  1.714086890220642\n",
      "Loss:  1.7033427953720093\n",
      "Loss:  1.588975429534912\n",
      "Loss:  1.6688013076782227\n",
      "Loss:  1.6561068296432495\n",
      "Loss:  1.7310106754302979\n",
      "Loss:  1.5775715112686157\n",
      "Loss:  1.6081515550613403\n",
      "Loss:  1.6118394136428833\n",
      "Loss:  1.6345547437667847\n",
      "Loss:  1.6801884174346924\n",
      "Loss:  1.551389217376709\n",
      "Loss:  1.60537850856781\n",
      "Loss:  1.6503093242645264\n",
      "Loss:  1.6916329860687256\n",
      "Loss:  1.7563745975494385\n",
      "Loss:  1.612025260925293\n",
      "Loss:  1.5976158380508423\n",
      "Loss:  1.6832078695297241\n",
      "Loss:  1.5675150156021118\n",
      "Loss:  1.5891642570495605\n",
      "Loss:  1.6176520586013794\n",
      "Loss:  1.5752363204956055\n",
      "Loss:  1.5683790445327759\n",
      "Loss:  1.6018755435943604\n",
      "Loss:  1.5486911535263062\n",
      "Loss:  1.6049541234970093\n",
      "Loss:  1.6404794454574585\n",
      "Loss:  1.6970136165618896\n",
      "Loss:  1.6200064420700073\n",
      "Loss:  1.5867230892181396\n",
      "Loss:  1.5779030323028564\n",
      "Loss:  1.7421233654022217\n",
      "Loss:  1.7248362302780151\n",
      "Loss:  1.5740966796875\n",
      "Loss:  1.6833059787750244\n",
      "Loss:  1.71062433719635\n",
      "Loss:  1.677311897277832\n",
      "Loss:  1.5876857042312622\n",
      "Loss:  1.5706740617752075\n",
      "Loss:  1.6155019998550415\n",
      "Loss:  1.5714372396469116\n",
      "Loss:  1.6873365640640259\n",
      "Loss:  1.685982584953308\n",
      "Loss:  1.651438593864441\n",
      "Loss:  1.6935548782348633\n",
      "Loss:  1.605532169342041\n",
      "Loss:  1.7477891445159912\n",
      "Loss:  1.6668444871902466\n",
      "Loss:  1.688916563987732\n",
      "Loss:  1.5973865985870361\n",
      "Loss:  1.637527585029602\n",
      "Loss:  1.5381194353103638\n",
      "Loss:  1.6055527925491333\n",
      "Loss:  1.601254940032959\n",
      "Loss:  1.607563853263855\n",
      "Loss:  1.6471318006515503\n",
      "Loss:  1.6132200956344604\n",
      "Loss:  1.609455943107605\n",
      "Loss:  1.6634671688079834\n",
      "Loss:  1.6972143650054932\n",
      "Loss:  1.6328511238098145\n",
      "Loss:  1.612337589263916\n",
      "Loss:  1.720116138458252\n",
      "Loss:  1.6506388187408447\n",
      "Loss:  1.6845957040786743\n",
      "Loss:  1.6581926345825195\n",
      "Loss:  1.5768083333969116\n",
      "Loss:  1.5670987367630005\n",
      "Loss:  1.5870137214660645\n",
      "Loss:  1.6982823610305786\n",
      "Loss:  1.6691652536392212\n",
      "Loss:  1.5691314935684204\n",
      "Loss:  1.6104172468185425\n",
      "Loss:  1.6754482984542847\n",
      "Loss:  1.6170774698257446\n",
      "Loss:  1.5850210189819336\n",
      "Loss:  1.7349591255187988\n",
      "Testing data accuracy: 55%\n",
      "Train Epoch: 3 Loss: 1.692149\n",
      "Train Epoch: 3 Loss: 1.683857\n",
      "Train Epoch: 3 Loss: 1.627233\n",
      "Train Epoch: 3 Loss: 1.535801\n",
      "Train Epoch: 3 Loss: 1.552099\n",
      "Train Epoch: 3 Loss: 1.666320\n",
      "Train Epoch: 3 Loss: 1.592071\n",
      "Train Epoch: 3 Loss: 1.465901\n",
      "Train Epoch: 3 Loss: 1.477475\n",
      "Train Epoch: 3 Loss: 1.479673\n",
      "Train Epoch: 3 Loss: 1.431909\n",
      "Train Epoch: 3 Loss: 1.500343\n",
      "Train Epoch: 3 Loss: 1.551331\n",
      "Train Epoch: 3 Loss: 1.517469\n",
      "Train Epoch: 3 Loss: 1.393033\n",
      "Train Epoch: 3 Loss: 1.595552\n",
      "Train Epoch: 3 Loss: 1.508247\n",
      "Train Epoch: 3 Loss: 1.500613\n",
      "Train Epoch: 3 Loss: 1.467369\n",
      "Train Epoch: 3 Loss: 1.364720\n",
      "Train Epoch: 3 Loss: 1.489118\n",
      "Train Epoch: 3 Loss: 1.436066\n",
      "Train Epoch: 3 Loss: 1.476255\n",
      "Train Epoch: 3 Loss: 1.278110\n",
      "Train Epoch: 3 Loss: 1.409441\n",
      "Train Epoch: 3 Loss: 1.334066\n",
      "Train Epoch: 3 Loss: 1.334677\n",
      "Train Epoch: 3 Loss: 1.416218\n",
      "Train Epoch: 3 Loss: 1.380248\n",
      "Train Epoch: 3 Loss: 1.256485\n",
      "Train Epoch: 3 Loss: 1.307709\n",
      "Train Epoch: 3 Loss: 1.241367\n",
      "Epoch:  3\n",
      "Loss:  1.2890424728393555\n",
      "Loss:  1.2380543947219849\n",
      "Loss:  1.3373382091522217\n",
      "Loss:  1.4459826946258545\n",
      "Loss:  1.4294910430908203\n",
      "Loss:  1.310096025466919\n",
      "Loss:  1.3243972063064575\n",
      "Loss:  1.3463637828826904\n",
      "Loss:  1.3010263442993164\n",
      "Loss:  1.457161784172058\n",
      "Loss:  1.271023154258728\n",
      "Loss:  1.3217657804489136\n",
      "Loss:  1.3914850950241089\n",
      "Loss:  1.460400938987732\n",
      "Loss:  1.3060755729675293\n",
      "Loss:  1.3437455892562866\n",
      "Loss:  1.3299860954284668\n",
      "Loss:  1.4243481159210205\n",
      "Loss:  1.3978843688964844\n",
      "Loss:  1.4358906745910645\n",
      "Loss:  1.3339427709579468\n",
      "Loss:  1.3643455505371094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  1.3682812452316284\n",
      "Loss:  1.3600116968154907\n",
      "Loss:  1.299446940422058\n",
      "Loss:  1.2850004434585571\n",
      "Loss:  1.4308342933654785\n",
      "Loss:  1.4051584005355835\n",
      "Loss:  1.354848027229309\n",
      "Loss:  1.4663701057434082\n",
      "Loss:  1.296506404876709\n",
      "Loss:  1.361981749534607\n",
      "Loss:  1.268383264541626\n",
      "Loss:  1.3148895502090454\n",
      "Loss:  1.3576604127883911\n",
      "Loss:  1.3316420316696167\n",
      "Loss:  1.251694917678833\n",
      "Loss:  1.3847038745880127\n",
      "Loss:  1.358306646347046\n",
      "Loss:  1.3265732526779175\n",
      "Loss:  1.3168174028396606\n",
      "Loss:  1.3091094493865967\n",
      "Loss:  1.2972437143325806\n",
      "Loss:  1.3136992454528809\n",
      "Loss:  1.546568512916565\n",
      "Loss:  1.3411272764205933\n",
      "Loss:  1.2672970294952393\n",
      "Loss:  1.334514856338501\n",
      "Loss:  1.363122582435608\n",
      "Loss:  1.2262904644012451\n",
      "Loss:  1.2581650018692017\n",
      "Loss:  1.280037522315979\n",
      "Loss:  1.4414751529693604\n",
      "Loss:  1.3536362648010254\n",
      "Loss:  1.3424617052078247\n",
      "Loss:  1.3587799072265625\n",
      "Loss:  1.3419653177261353\n",
      "Loss:  1.3872630596160889\n",
      "Loss:  1.32968270778656\n",
      "Loss:  1.338807463645935\n",
      "Loss:  1.3791239261627197\n",
      "Loss:  1.3252142667770386\n",
      "Loss:  1.2768205404281616\n",
      "Loss:  1.4471899271011353\n",
      "Loss:  1.358499526977539\n",
      "Loss:  1.4148529767990112\n",
      "Loss:  1.3137454986572266\n",
      "Loss:  1.341123342514038\n",
      "Loss:  1.2690495252609253\n",
      "Loss:  1.3570860624313354\n",
      "Loss:  1.380520224571228\n",
      "Loss:  1.3798447847366333\n",
      "Loss:  1.351073980331421\n",
      "Loss:  1.320114016532898\n",
      "Loss:  1.374605655670166\n",
      "Loss:  1.3479273319244385\n",
      "Loss:  1.2121615409851074\n",
      "Loss:  1.2419123649597168\n",
      "Loss:  1.3512581586837769\n",
      "Loss:  1.3195923566818237\n",
      "Loss:  1.3198456764221191\n",
      "Loss:  1.3501595258712769\n",
      "Loss:  1.3714048862457275\n",
      "Loss:  1.4672586917877197\n",
      "Loss:  1.3102940320968628\n",
      "Loss:  1.3492424488067627\n",
      "Loss:  1.3901033401489258\n",
      "Loss:  1.3944785594940186\n",
      "Loss:  1.2062008380889893\n",
      "Loss:  1.349149227142334\n",
      "Loss:  1.388754963874817\n",
      "Loss:  1.2917379140853882\n",
      "Loss:  1.320082664489746\n",
      "Loss:  1.2684649229049683\n",
      "Loss:  1.3399298191070557\n",
      "Loss:  1.3447072505950928\n",
      "Loss:  1.427542805671692\n",
      "Loss:  1.2938258647918701\n",
      "Loss:  1.4402273893356323\n",
      "Loss:  1.3349356651306152\n",
      "Loss:  1.192832112312317\n",
      "Loss:  1.3156569004058838\n",
      "Loss:  1.3770551681518555\n",
      "Loss:  1.2461460828781128\n",
      "Loss:  1.3476450443267822\n",
      "Loss:  1.4651036262512207\n",
      "Loss:  1.2791072130203247\n",
      "Loss:  1.3924391269683838\n",
      "Loss:  1.3405203819274902\n",
      "Loss:  1.2866981029510498\n",
      "Loss:  1.356403112411499\n",
      "Loss:  1.3396894931793213\n",
      "Loss:  1.3223546743392944\n",
      "Loss:  1.3451366424560547\n",
      "Loss:  1.4010741710662842\n",
      "Loss:  1.3112858533859253\n",
      "Loss:  1.3992866277694702\n",
      "Loss:  1.2475841045379639\n",
      "Loss:  1.4807257652282715\n",
      "Loss:  1.3735604286193848\n",
      "Loss:  1.3327690362930298\n",
      "Loss:  1.3123973608016968\n",
      "Loss:  1.3869459629058838\n",
      "Loss:  1.4119012355804443\n",
      "Loss:  1.355879783630371\n",
      "Loss:  1.2330858707427979\n",
      "Loss:  1.322576880455017\n",
      "Loss:  1.311833381652832\n",
      "Loss:  1.4346739053726196\n",
      "Loss:  1.2857893705368042\n",
      "Loss:  1.4192020893096924\n",
      "Loss:  1.4057267904281616\n",
      "Loss:  1.364749789237976\n",
      "Loss:  1.3753606081008911\n",
      "Loss:  1.4268804788589478\n",
      "Loss:  1.3888802528381348\n",
      "Loss:  1.3538532257080078\n",
      "Loss:  1.315081000328064\n",
      "Loss:  1.3084806203842163\n",
      "Loss:  1.3717817068099976\n",
      "Loss:  1.302193522453308\n",
      "Loss:  1.3458824157714844\n",
      "Loss:  1.3874058723449707\n",
      "Loss:  1.31714928150177\n",
      "Loss:  1.2486048936843872\n",
      "Loss:  1.327288031578064\n",
      "Loss:  1.2365975379943848\n",
      "Loss:  1.3575594425201416\n",
      "Loss:  1.2269432544708252\n",
      "Loss:  1.2110234498977661\n",
      "Loss:  1.3223756551742554\n",
      "Loss:  1.368592381477356\n",
      "Loss:  1.3268698453903198\n",
      "Loss:  1.2337708473205566\n",
      "Loss:  1.4003554582595825\n",
      "Loss:  1.2494957447052002\n",
      "Loss:  1.3343271017074585\n",
      "Testing data accuracy: 64%\n",
      "Train Epoch: 4 Loss: 1.303603\n",
      "Train Epoch: 4 Loss: 1.339230\n",
      "Train Epoch: 4 Loss: 1.266513\n",
      "Train Epoch: 4 Loss: 1.234769\n",
      "Train Epoch: 4 Loss: 1.215904\n",
      "Train Epoch: 4 Loss: 1.271503\n",
      "Train Epoch: 4 Loss: 1.246858\n",
      "Train Epoch: 4 Loss: 1.312898\n",
      "Train Epoch: 4 Loss: 1.314107\n",
      "Train Epoch: 4 Loss: 1.389487\n",
      "Train Epoch: 4 Loss: 1.232787\n",
      "Train Epoch: 4 Loss: 1.375134\n",
      "Train Epoch: 4 Loss: 1.268752\n",
      "Train Epoch: 4 Loss: 1.194872\n",
      "Train Epoch: 4 Loss: 1.329222\n",
      "Train Epoch: 4 Loss: 1.302269\n",
      "Train Epoch: 4 Loss: 1.156013\n",
      "Train Epoch: 4 Loss: 1.140381\n",
      "Train Epoch: 4 Loss: 1.264620\n",
      "Train Epoch: 4 Loss: 1.204687\n",
      "Train Epoch: 4 Loss: 1.148102\n",
      "Train Epoch: 4 Loss: 1.329367\n",
      "Train Epoch: 4 Loss: 1.244425\n",
      "Train Epoch: 4 Loss: 1.096139\n",
      "Train Epoch: 4 Loss: 1.313085\n",
      "Train Epoch: 4 Loss: 1.035317\n",
      "Train Epoch: 4 Loss: 1.152690\n",
      "Train Epoch: 4 Loss: 1.150779\n",
      "Train Epoch: 4 Loss: 1.225044\n",
      "Train Epoch: 4 Loss: 1.203059\n",
      "Train Epoch: 4 Loss: 1.062000\n",
      "Train Epoch: 4 Loss: 1.134078\n",
      "Epoch:  4\n",
      "Loss:  1.077163815498352\n",
      "Loss:  1.10715651512146\n",
      "Loss:  1.0688472986221313\n",
      "Loss:  1.1148210763931274\n",
      "Loss:  1.264182686805725\n",
      "Loss:  1.202101469039917\n",
      "Loss:  1.1613019704818726\n",
      "Loss:  1.0964031219482422\n",
      "Loss:  1.1780993938446045\n",
      "Loss:  1.1664155721664429\n",
      "Loss:  1.0734424591064453\n",
      "Loss:  1.2041771411895752\n",
      "Loss:  1.1804555654525757\n",
      "Loss:  1.1949464082717896\n",
      "Loss:  1.0861244201660156\n",
      "Loss:  1.1486055850982666\n",
      "Loss:  1.143998146057129\n",
      "Loss:  1.1016654968261719\n",
      "Loss:  1.1893906593322754\n",
      "Loss:  1.1059457063674927\n",
      "Loss:  1.1341174840927124\n",
      "Loss:  1.162318468093872\n",
      "Loss:  1.022957444190979\n",
      "Loss:  1.1201164722442627\n",
      "Loss:  1.0976202487945557\n",
      "Loss:  1.0044364929199219\n",
      "Loss:  1.188899278640747\n",
      "Loss:  1.0963598489761353\n",
      "Loss:  1.1294002532958984\n",
      "Loss:  1.2108101844787598\n",
      "Loss:  1.210853934288025\n",
      "Loss:  1.170253038406372\n",
      "Loss:  1.097155213356018\n",
      "Loss:  1.2275739908218384\n",
      "Loss:  1.0489383935928345\n",
      "Loss:  1.0944905281066895\n",
      "Loss:  1.2064276933670044\n",
      "Loss:  1.2028687000274658\n",
      "Loss:  1.105242371559143\n",
      "Loss:  1.126190185546875\n",
      "Loss:  1.0223134756088257\n",
      "Loss:  1.3324025869369507\n",
      "Loss:  1.2012760639190674\n",
      "Loss:  1.0459063053131104\n",
      "Loss:  1.0174102783203125\n",
      "Loss:  1.166754961013794\n",
      "Loss:  1.1521295309066772\n",
      "Loss:  1.2539992332458496\n",
      "Loss:  1.2024959325790405\n",
      "Loss:  1.122051477432251\n",
      "Loss:  1.1207367181777954\n",
      "Loss:  1.1428680419921875\n",
      "Loss:  1.1542857885360718\n",
      "Loss:  1.1258550882339478\n",
      "Loss:  1.2702018022537231\n",
      "Loss:  1.125365138053894\n",
      "Loss:  1.0057991743087769\n",
      "Loss:  1.3493586778640747\n",
      "Loss:  1.063070297241211\n",
      "Loss:  1.0866893529891968\n",
      "Loss:  1.1704955101013184\n",
      "Loss:  1.1484216451644897\n",
      "Loss:  1.0220218896865845\n",
      "Loss:  1.1668838262557983\n",
      "Loss:  1.1140122413635254\n",
      "Loss:  1.0706787109375\n",
      "Loss:  1.1765133142471313\n",
      "Loss:  1.0685486793518066\n",
      "Loss:  1.081614375114441\n",
      "Loss:  1.0818415880203247\n",
      "Loss:  1.1641517877578735\n",
      "Loss:  1.06771719455719\n",
      "Loss:  1.1943432092666626\n",
      "Loss:  1.0821919441223145\n",
      "Loss:  1.2248291969299316\n",
      "Loss:  1.1168879270553589\n",
      "Loss:  1.1173748970031738\n",
      "Loss:  1.2303791046142578\n",
      "Loss:  1.2036149501800537\n",
      "Loss:  1.2560570240020752\n",
      "Loss:  1.1261426210403442\n",
      "Loss:  1.2371900081634521\n",
      "Loss:  1.0733156204223633\n",
      "Loss:  1.084707498550415\n",
      "Loss:  1.1246520280838013\n",
      "Loss:  1.2735624313354492\n",
      "Loss:  1.0973243713378906\n",
      "Loss:  1.1579378843307495\n",
      "Loss:  1.149268627166748\n",
      "Loss:  1.1492998600006104\n",
      "Loss:  1.1922041177749634\n",
      "Loss:  1.2549453973770142\n",
      "Loss:  1.1082020998001099\n",
      "Loss:  1.2397176027297974\n",
      "Loss:  1.208726167678833\n",
      "Loss:  1.2728416919708252\n",
      "Loss:  1.133023738861084\n",
      "Loss:  1.2756448984146118\n",
      "Loss:  1.1269102096557617\n",
      "Loss:  1.1038979291915894\n",
      "Loss:  0.959315299987793\n",
      "Loss:  1.202322244644165\n",
      "Loss:  1.1232417821884155\n",
      "Loss:  1.1875534057617188\n",
      "Loss:  1.1411855220794678\n",
      "Loss:  1.2169692516326904\n",
      "Loss:  1.0192922353744507\n",
      "Loss:  1.1177064180374146\n",
      "Loss:  1.165012240409851\n",
      "Loss:  1.1227679252624512\n",
      "Loss:  1.1742172241210938\n",
      "Loss:  1.187666416168213\n",
      "Loss:  1.2071924209594727\n",
      "Loss:  1.0932559967041016\n",
      "Loss:  1.2467154264450073\n",
      "Loss:  1.069683313369751\n",
      "Loss:  1.1215085983276367\n",
      "Loss:  1.0539679527282715\n",
      "Loss:  1.0567792654037476\n",
      "Loss:  1.1770248413085938\n",
      "Loss:  1.3013410568237305\n",
      "Loss:  1.1002336740493774\n",
      "Loss:  1.2293027639389038\n",
      "Loss:  1.0386496782302856\n",
      "Loss:  1.1972469091415405\n",
      "Loss:  1.0665534734725952\n",
      "Loss:  1.1555116176605225\n",
      "Loss:  1.077480673789978\n",
      "Loss:  1.147854208946228\n",
      "Loss:  1.1350336074829102\n",
      "Loss:  1.133569598197937\n",
      "Loss:  1.0099568367004395\n",
      "Loss:  1.2321377992630005\n",
      "Loss:  1.231336236000061\n",
      "Loss:  1.2146329879760742\n",
      "Loss:  1.2530633211135864\n",
      "Loss:  1.1189281940460205\n",
      "Loss:  1.1397687196731567\n",
      "Loss:  1.1812114715576172\n",
      "Loss:  1.2144415378570557\n",
      "Loss:  1.1549921035766602\n",
      "Loss:  1.0523946285247803\n",
      "Loss:  1.1789511442184448\n",
      "Loss:  1.2576346397399902\n",
      "Loss:  1.1012071371078491\n",
      "Loss:  1.1451349258422852\n",
      "Loss:  1.158812165260315\n",
      "Loss:  1.1803821325302124\n",
      "Loss:  1.171977162361145\n",
      "Loss:  1.2188842296600342\n",
      "Loss:  1.1639564037322998\n",
      "Loss:  1.1714091300964355\n",
      "Loss:  1.184624433517456\n",
      "Loss:  1.1286203861236572\n",
      "Loss:  1.1278257369995117\n",
      "Loss:  1.139308214187622\n",
      "Loss:  1.2588728666305542\n",
      "Testing data accuracy: 68%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 Loss: 1.274149\n",
      "Train Epoch: 5 Loss: 1.211354\n",
      "Train Epoch: 5 Loss: 1.078285\n",
      "Train Epoch: 5 Loss: 1.144699\n",
      "Train Epoch: 5 Loss: 1.109192\n",
      "Train Epoch: 5 Loss: 0.992682\n",
      "Train Epoch: 5 Loss: 0.993411\n",
      "Train Epoch: 5 Loss: 1.237637\n",
      "Train Epoch: 5 Loss: 1.094409\n",
      "Train Epoch: 5 Loss: 1.198745\n",
      "Train Epoch: 5 Loss: 1.008521\n",
      "Train Epoch: 5 Loss: 1.134951\n",
      "Train Epoch: 5 Loss: 1.049276\n",
      "Train Epoch: 5 Loss: 1.060020\n",
      "Train Epoch: 5 Loss: 1.222608\n",
      "Train Epoch: 5 Loss: 1.038183\n",
      "Train Epoch: 5 Loss: 1.069355\n",
      "Train Epoch: 5 Loss: 1.008977\n",
      "Train Epoch: 5 Loss: 1.042935\n",
      "Train Epoch: 5 Loss: 1.096052\n",
      "Train Epoch: 5 Loss: 0.983311\n",
      "Train Epoch: 5 Loss: 1.134735\n",
      "Train Epoch: 5 Loss: 1.052598\n",
      "Train Epoch: 5 Loss: 1.084564\n",
      "Train Epoch: 5 Loss: 1.106061\n",
      "Train Epoch: 5 Loss: 0.932964\n",
      "Train Epoch: 5 Loss: 0.972498\n",
      "Train Epoch: 5 Loss: 1.080489\n",
      "Train Epoch: 5 Loss: 0.997506\n",
      "Train Epoch: 5 Loss: 1.109815\n",
      "Train Epoch: 5 Loss: 0.942182\n",
      "Train Epoch: 5 Loss: 0.933815\n",
      "Epoch:  5\n",
      "Loss:  0.9779365062713623\n",
      "Loss:  1.0659345388412476\n",
      "Loss:  1.0289465188980103\n",
      "Loss:  1.0162650346755981\n",
      "Loss:  1.0626282691955566\n",
      "Loss:  1.0917885303497314\n",
      "Loss:  1.0087629556655884\n",
      "Loss:  0.8599703907966614\n",
      "Loss:  1.0784761905670166\n",
      "Loss:  0.9661023020744324\n",
      "Loss:  1.0353424549102783\n",
      "Loss:  1.064178705215454\n",
      "Loss:  1.2227262258529663\n",
      "Loss:  1.0104604959487915\n",
      "Loss:  0.9451324939727783\n",
      "Loss:  0.9152708053588867\n",
      "Loss:  1.135440707206726\n",
      "Loss:  0.9922055602073669\n",
      "Loss:  0.9898332953453064\n",
      "Loss:  1.0192220211029053\n",
      "Loss:  1.0015476942062378\n",
      "Loss:  1.0493415594100952\n",
      "Loss:  0.95914626121521\n",
      "Loss:  1.0688570737838745\n",
      "Loss:  0.9544444680213928\n",
      "Loss:  1.0465168952941895\n",
      "Loss:  0.9949169754981995\n",
      "Loss:  1.0994435548782349\n",
      "Loss:  1.0406399965286255\n",
      "Loss:  1.0868291854858398\n",
      "Loss:  1.0319530963897705\n",
      "Loss:  1.127575397491455\n",
      "Loss:  1.0800973176956177\n",
      "Loss:  1.1297191381454468\n",
      "Loss:  0.9507928490638733\n",
      "Loss:  1.069082498550415\n",
      "Loss:  0.9345448613166809\n",
      "Loss:  0.9955627918243408\n",
      "Loss:  1.0243321657180786\n",
      "Loss:  1.0570783615112305\n",
      "Loss:  1.027214765548706\n",
      "Loss:  1.1390897035598755\n",
      "Loss:  0.9265671372413635\n",
      "Loss:  0.9813657999038696\n",
      "Loss:  1.073042631149292\n",
      "Loss:  0.9284218549728394\n",
      "Loss:  0.9986650943756104\n",
      "Loss:  0.9738937020301819\n",
      "Loss:  0.9519181251525879\n",
      "Loss:  1.1312154531478882\n",
      "Loss:  1.1190271377563477\n",
      "Loss:  1.0452473163604736\n",
      "Loss:  1.1381964683532715\n",
      "Loss:  0.9592574834823608\n",
      "Loss:  0.9705082178115845\n",
      "Loss:  1.0095791816711426\n",
      "Loss:  0.9724213480949402\n",
      "Loss:  1.0144232511520386\n",
      "Loss:  0.993013858795166\n",
      "Loss:  0.9508949518203735\n",
      "Loss:  0.9980409741401672\n",
      "Loss:  0.9523167610168457\n",
      "Loss:  1.0912901163101196\n",
      "Loss:  1.0912854671478271\n",
      "Loss:  1.0975128412246704\n",
      "Loss:  0.9840371608734131\n",
      "Loss:  1.0785646438598633\n",
      "Loss:  0.9226264953613281\n",
      "Loss:  1.1172847747802734\n",
      "Loss:  1.0089826583862305\n",
      "Loss:  0.9426557421684265\n",
      "Loss:  0.9011258482933044\n",
      "Loss:  0.9763408303260803\n",
      "Loss:  0.8080398440361023\n",
      "Loss:  0.9355552792549133\n",
      "Loss:  0.897232174873352\n",
      "Loss:  1.0269614458084106\n",
      "Loss:  1.0270466804504395\n",
      "Loss:  0.9114993810653687\n",
      "Loss:  1.032718300819397\n",
      "Loss:  1.0182888507843018\n",
      "Loss:  1.0461673736572266\n",
      "Loss:  1.094138264656067\n",
      "Loss:  1.029189944267273\n",
      "Loss:  1.0446511507034302\n",
      "Loss:  1.0664509534835815\n",
      "Loss:  1.006372332572937\n",
      "Loss:  1.0120218992233276\n",
      "Loss:  1.0527145862579346\n",
      "Loss:  0.9581716656684875\n",
      "Loss:  0.9375549554824829\n",
      "Loss:  1.1048405170440674\n",
      "Loss:  0.9808049201965332\n",
      "Loss:  1.003887414932251\n",
      "Loss:  1.0372196435928345\n",
      "Loss:  1.0162981748580933\n",
      "Loss:  0.9849117994308472\n",
      "Loss:  1.081140398979187\n",
      "Loss:  0.9801400303840637\n",
      "Loss:  0.9144597053527832\n",
      "Loss:  1.0433478355407715\n",
      "Loss:  0.9778717756271362\n",
      "Loss:  1.049734354019165\n",
      "Loss:  1.010922908782959\n",
      "Loss:  1.0364973545074463\n",
      "Loss:  0.9610695242881775\n",
      "Loss:  1.0575324296951294\n",
      "Loss:  0.9505947232246399\n",
      "Loss:  1.0675702095031738\n",
      "Loss:  1.0728670358657837\n",
      "Loss:  0.9684947729110718\n",
      "Loss:  1.0741305351257324\n",
      "Loss:  1.0655544996261597\n",
      "Loss:  1.123213291168213\n",
      "Loss:  1.0838420391082764\n",
      "Loss:  1.0495500564575195\n",
      "Loss:  1.0288246870040894\n",
      "Loss:  0.9912540912628174\n",
      "Loss:  1.1168091297149658\n",
      "Loss:  1.044889211654663\n",
      "Loss:  0.9920356869697571\n",
      "Loss:  1.0168728828430176\n",
      "Loss:  0.9749147891998291\n",
      "Loss:  0.9854174256324768\n",
      "Loss:  1.174877643585205\n",
      "Loss:  0.9876025319099426\n",
      "Loss:  1.1186457872390747\n",
      "Loss:  1.0274373292922974\n",
      "Loss:  1.021854281425476\n",
      "Loss:  1.0223006010055542\n",
      "Loss:  1.1208782196044922\n",
      "Loss:  0.890060305595398\n",
      "Loss:  1.0143858194351196\n",
      "Loss:  1.0278595685958862\n",
      "Loss:  1.0134353637695312\n",
      "Loss:  0.9947485327720642\n",
      "Loss:  1.0560531616210938\n",
      "Loss:  1.0145293474197388\n",
      "Loss:  1.1183812618255615\n",
      "Loss:  0.9852607846260071\n",
      "Loss:  0.876306414604187\n",
      "Loss:  1.0680792331695557\n",
      "Loss:  1.103635311126709\n",
      "Loss:  0.9268235564231873\n",
      "Loss:  0.8937163949012756\n",
      "Loss:  1.006537914276123\n",
      "Loss:  0.9555999040603638\n",
      "Loss:  0.8888342380523682\n",
      "Loss:  1.0774613618850708\n",
      "Loss:  0.9070823192596436\n",
      "Loss:  1.132677435874939\n",
      "Loss:  1.025447130203247\n",
      "Loss:  0.9349634051322937\n",
      "Loss:  0.9857563972473145\n",
      "Loss:  0.859046995639801\n",
      "Loss:  1.0048120021820068\n",
      "Loss:  0.9540260434150696\n",
      "Testing data accuracy: 71%\n",
      "Train Epoch: 6 Loss: 1.064490\n",
      "Train Epoch: 6 Loss: 0.975685\n",
      "Train Epoch: 6 Loss: 0.948052\n",
      "Train Epoch: 6 Loss: 1.055767\n",
      "Train Epoch: 6 Loss: 0.873151\n",
      "Train Epoch: 6 Loss: 0.989558\n",
      "Train Epoch: 6 Loss: 1.003687\n",
      "Train Epoch: 6 Loss: 1.087125\n",
      "Train Epoch: 6 Loss: 1.097569\n",
      "Train Epoch: 6 Loss: 0.855843\n",
      "Train Epoch: 6 Loss: 1.025607\n",
      "Train Epoch: 6 Loss: 0.918400\n",
      "Train Epoch: 6 Loss: 1.075882\n",
      "Train Epoch: 6 Loss: 1.142801\n",
      "Train Epoch: 6 Loss: 1.261376\n",
      "Train Epoch: 6 Loss: 1.000401\n",
      "Train Epoch: 6 Loss: 0.877192\n",
      "Train Epoch: 6 Loss: 0.981421\n",
      "Train Epoch: 6 Loss: 0.956737\n",
      "Train Epoch: 6 Loss: 1.003232\n",
      "Train Epoch: 6 Loss: 0.813194\n",
      "Train Epoch: 6 Loss: 1.019888\n",
      "Train Epoch: 6 Loss: 0.980500\n",
      "Train Epoch: 6 Loss: 0.889127\n",
      "Train Epoch: 6 Loss: 0.905823\n",
      "Train Epoch: 6 Loss: 0.972374\n",
      "Train Epoch: 6 Loss: 0.889517\n",
      "Train Epoch: 6 Loss: 0.868081\n",
      "Train Epoch: 6 Loss: 0.911379\n",
      "Train Epoch: 6 Loss: 0.852901\n",
      "Train Epoch: 6 Loss: 0.964648\n",
      "Train Epoch: 6 Loss: 0.970765\n",
      "Epoch:  6\n",
      "Loss:  0.7834196090698242\n",
      "Loss:  0.9868265986442566\n",
      "Loss:  0.794074535369873\n",
      "Loss:  0.9216629862785339\n",
      "Loss:  0.9997682571411133\n",
      "Loss:  0.9549126625061035\n",
      "Loss:  0.9791678786277771\n",
      "Loss:  0.9868866801261902\n",
      "Loss:  0.9314836859703064\n",
      "Loss:  0.9687812924385071\n",
      "Loss:  0.848647952079773\n",
      "Loss:  0.950358510017395\n",
      "Loss:  0.9817774295806885\n",
      "Loss:  0.945551335811615\n",
      "Loss:  0.785308837890625\n",
      "Loss:  0.8559053540229797\n",
      "Loss:  0.8124170899391174\n",
      "Loss:  0.9314272403717041\n",
      "Loss:  0.9909460544586182\n",
      "Loss:  0.8994565010070801\n",
      "Loss:  0.9283082485198975\n",
      "Loss:  0.8264878392219543\n",
      "Loss:  1.0352736711502075\n",
      "Loss:  0.8626541495323181\n",
      "Loss:  0.9570193886756897\n",
      "Loss:  0.8845601081848145\n",
      "Loss:  0.9108182787895203\n",
      "Loss:  0.9503070116043091\n",
      "Loss:  1.0360617637634277\n",
      "Loss:  0.9959297776222229\n",
      "Loss:  0.9261720776557922\n",
      "Loss:  0.9033299088478088\n",
      "Loss:  0.8571714758872986\n",
      "Loss:  0.7696989178657532\n",
      "Loss:  0.8510317802429199\n",
      "Loss:  0.9298840165138245\n",
      "Loss:  0.7715274095535278\n",
      "Loss:  1.0791213512420654\n",
      "Loss:  0.8492845296859741\n",
      "Loss:  0.895832896232605\n",
      "Loss:  0.986122727394104\n",
      "Loss:  1.0246094465255737\n",
      "Loss:  0.7243865728378296\n",
      "Loss:  0.9017584919929504\n",
      "Loss:  0.9148746132850647\n",
      "Loss:  0.8206323385238647\n",
      "Loss:  0.9969998002052307\n",
      "Loss:  0.9595115780830383\n",
      "Loss:  0.9716449975967407\n",
      "Loss:  0.9156979322433472\n",
      "Loss:  0.9512495398521423\n",
      "Loss:  0.9677553772926331\n",
      "Loss:  0.8991116285324097\n",
      "Loss:  0.8960644602775574\n",
      "Loss:  0.9436440467834473\n",
      "Loss:  0.8828215599060059\n",
      "Loss:  0.811338484287262\n",
      "Loss:  0.9123153686523438\n",
      "Loss:  0.7075825929641724\n",
      "Loss:  0.9720379114151001\n",
      "Loss:  0.8175857663154602\n",
      "Loss:  1.0384353399276733\n",
      "Loss:  0.8764247298240662\n",
      "Loss:  0.996367871761322\n",
      "Loss:  0.9554755687713623\n",
      "Loss:  0.9320794343948364\n",
      "Loss:  0.941619336605072\n",
      "Loss:  0.8336681127548218\n",
      "Loss:  0.8307071924209595\n",
      "Loss:  0.9147427082061768\n",
      "Loss:  0.8763594031333923\n",
      "Loss:  1.0934432744979858\n",
      "Loss:  1.017391562461853\n",
      "Loss:  0.9389392137527466\n",
      "Loss:  0.9604938626289368\n",
      "Loss:  0.8388396501541138\n",
      "Loss:  1.1122523546218872\n",
      "Loss:  0.9667897820472717\n",
      "Loss:  0.8259328007698059\n",
      "Loss:  0.9365328550338745\n",
      "Loss:  0.9323434829711914\n",
      "Loss:  0.9444892406463623\n",
      "Loss:  0.9183811545372009\n",
      "Loss:  0.8908938765525818\n",
      "Loss:  0.8225870728492737\n",
      "Loss:  0.8712612390518188\n",
      "Loss:  0.7861037850379944\n",
      "Loss:  0.8616600632667542\n",
      "Loss:  0.988968014717102\n",
      "Loss:  0.8839846253395081\n",
      "Loss:  0.8393412232398987\n",
      "Loss:  0.8239885568618774\n",
      "Loss:  0.9885908365249634\n",
      "Loss:  0.9336603879928589\n",
      "Loss:  0.9051491618156433\n",
      "Loss:  0.9073919057846069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.8733068108558655\n",
      "Loss:  0.868728518486023\n",
      "Loss:  0.8699883222579956\n",
      "Loss:  0.879367470741272\n",
      "Loss:  0.8954070806503296\n",
      "Loss:  0.9867135286331177\n",
      "Loss:  0.8090047836303711\n",
      "Loss:  0.8465895056724548\n",
      "Loss:  0.8966426253318787\n",
      "Loss:  0.7453280687332153\n",
      "Loss:  0.9169443845748901\n",
      "Loss:  0.8683233857154846\n",
      "Loss:  0.9493838548660278\n",
      "Loss:  0.883435070514679\n",
      "Loss:  1.092605710029602\n",
      "Loss:  1.0410104990005493\n",
      "Loss:  0.9090010523796082\n",
      "Loss:  1.0307209491729736\n",
      "Loss:  1.0857439041137695\n",
      "Loss:  0.8460127115249634\n",
      "Loss:  1.025457501411438\n",
      "Loss:  0.9008001089096069\n",
      "Loss:  0.9233028292655945\n",
      "Loss:  0.9659305810928345\n",
      "Loss:  0.8858029246330261\n",
      "Loss:  1.0356545448303223\n",
      "Loss:  0.9525965452194214\n",
      "Loss:  0.9247839450836182\n",
      "Loss:  0.9374855756759644\n",
      "Loss:  0.9349952936172485\n",
      "Loss:  0.946402370929718\n",
      "Loss:  0.9975140690803528\n",
      "Loss:  0.9902721047401428\n",
      "Loss:  1.0225012302398682\n",
      "Loss:  0.9519771933555603\n",
      "Loss:  0.8056424260139465\n",
      "Loss:  0.8564870953559875\n",
      "Loss:  0.9974485039710999\n",
      "Loss:  1.0476672649383545\n",
      "Loss:  0.8770250082015991\n",
      "Loss:  0.9691045880317688\n",
      "Loss:  1.2293366193771362\n",
      "Loss:  0.9598267078399658\n",
      "Loss:  0.8451419472694397\n",
      "Loss:  0.8963239789009094\n",
      "Loss:  0.8084267377853394\n",
      "Loss:  1.0204545259475708\n",
      "Loss:  0.9410982728004456\n",
      "Loss:  1.0985519886016846\n",
      "Loss:  0.9419130682945251\n",
      "Loss:  0.9635827541351318\n",
      "Loss:  1.0860650539398193\n",
      "Loss:  0.9037429690361023\n",
      "Loss:  0.8578101992607117\n",
      "Loss:  0.9625527262687683\n",
      "Loss:  0.8446091413497925\n",
      "Loss:  0.9984675049781799\n",
      "Loss:  1.078231692314148\n",
      "Loss:  0.9652543663978577\n",
      "Loss:  1.1277174949645996\n",
      "Loss:  0.9790450930595398\n",
      "Testing data accuracy: 72%\n",
      "Train Epoch: 7 Loss: 0.970209\n",
      "Train Epoch: 7 Loss: 0.735671\n",
      "Train Epoch: 7 Loss: 0.947805\n",
      "Train Epoch: 7 Loss: 0.960925\n",
      "Train Epoch: 7 Loss: 0.878097\n",
      "Train Epoch: 7 Loss: 0.910228\n",
      "Train Epoch: 7 Loss: 0.916155\n",
      "Train Epoch: 7 Loss: 0.874072\n",
      "Train Epoch: 7 Loss: 0.787500\n",
      "Train Epoch: 7 Loss: 0.784228\n",
      "Train Epoch: 7 Loss: 0.825748\n",
      "Train Epoch: 7 Loss: 0.953773\n",
      "Train Epoch: 7 Loss: 0.833809\n",
      "Train Epoch: 7 Loss: 0.908877\n",
      "Train Epoch: 7 Loss: 0.794254\n",
      "Train Epoch: 7 Loss: 0.952468\n",
      "Train Epoch: 7 Loss: 0.982641\n",
      "Train Epoch: 7 Loss: 0.768839\n",
      "Train Epoch: 7 Loss: 0.868588\n",
      "Train Epoch: 7 Loss: 1.012812\n",
      "Train Epoch: 7 Loss: 0.861666\n",
      "Train Epoch: 7 Loss: 0.986859\n",
      "Train Epoch: 7 Loss: 0.790382\n",
      "Train Epoch: 7 Loss: 0.877733\n",
      "Train Epoch: 7 Loss: 0.888401\n",
      "Train Epoch: 7 Loss: 0.961595\n",
      "Train Epoch: 7 Loss: 0.934271\n",
      "Train Epoch: 7 Loss: 0.750436\n",
      "Train Epoch: 7 Loss: 0.895432\n",
      "Train Epoch: 7 Loss: 0.777309\n",
      "Train Epoch: 7 Loss: 0.789763\n",
      "Train Epoch: 7 Loss: 0.761235\n",
      "Epoch:  7\n",
      "Loss:  0.8993463516235352\n",
      "Loss:  0.9062463045120239\n",
      "Loss:  0.9516320824623108\n",
      "Loss:  0.8815964460372925\n",
      "Loss:  0.7513989210128784\n",
      "Loss:  1.0082380771636963\n",
      "Loss:  0.8715817928314209\n",
      "Loss:  0.8264990448951721\n",
      "Loss:  0.8368632793426514\n",
      "Loss:  0.8393007516860962\n",
      "Loss:  0.8814100027084351\n",
      "Loss:  0.8219896554946899\n",
      "Loss:  0.8119445443153381\n",
      "Loss:  0.7621505260467529\n",
      "Loss:  0.7680253386497498\n",
      "Loss:  0.8236911296844482\n",
      "Loss:  0.8508070111274719\n",
      "Loss:  0.8782632946968079\n",
      "Loss:  0.815319299697876\n",
      "Loss:  0.9424429535865784\n",
      "Loss:  0.9162654876708984\n",
      "Loss:  0.8625162243843079\n",
      "Loss:  0.9249683618545532\n",
      "Loss:  0.8063511848449707\n",
      "Loss:  0.7686521410942078\n",
      "Loss:  0.8273246884346008\n",
      "Loss:  0.7726916670799255\n",
      "Loss:  0.9326262474060059\n",
      "Loss:  0.8299165964126587\n",
      "Loss:  0.7804019451141357\n",
      "Loss:  0.8917191028594971\n",
      "Loss:  0.9004843831062317\n",
      "Loss:  0.8712241649627686\n",
      "Loss:  0.9578614830970764\n",
      "Loss:  0.9474491477012634\n",
      "Loss:  1.0081120729446411\n",
      "Loss:  0.9296171069145203\n",
      "Loss:  0.8960241675376892\n",
      "Loss:  0.8312011957168579\n",
      "Loss:  0.7840451002120972\n",
      "Loss:  0.973928689956665\n",
      "Loss:  0.913844645023346\n",
      "Loss:  0.8410677909851074\n",
      "Loss:  0.9923198223114014\n",
      "Loss:  0.992989182472229\n",
      "Loss:  0.8380778431892395\n",
      "Loss:  0.7710161209106445\n",
      "Loss:  0.8796759843826294\n",
      "Loss:  0.8241112232208252\n",
      "Loss:  0.922274112701416\n",
      "Loss:  0.8675122261047363\n",
      "Loss:  0.8597084283828735\n",
      "Loss:  0.9035277366638184\n",
      "Loss:  0.790703535079956\n",
      "Loss:  0.8609529733657837\n",
      "Loss:  0.9383387565612793\n",
      "Loss:  0.7287164330482483\n",
      "Loss:  1.086350679397583\n",
      "Loss:  0.9186350107192993\n",
      "Loss:  0.7990148067474365\n",
      "Loss:  0.8220721483230591\n",
      "Loss:  0.836098849773407\n",
      "Loss:  0.8132954239845276\n",
      "Loss:  0.953984797000885\n",
      "Loss:  0.8284153342247009\n",
      "Loss:  0.8439749479293823\n",
      "Loss:  0.8073399066925049\n",
      "Loss:  0.827397882938385\n",
      "Loss:  0.801554262638092\n",
      "Loss:  0.8355614542961121\n",
      "Loss:  0.8966389894485474\n",
      "Loss:  1.0019268989562988\n",
      "Loss:  0.9269533157348633\n",
      "Loss:  0.8287686705589294\n",
      "Loss:  0.7705961465835571\n",
      "Loss:  0.8488886952400208\n",
      "Loss:  0.7577131986618042\n",
      "Loss:  0.8803819417953491\n",
      "Loss:  0.955795407295227\n",
      "Loss:  0.835697591304779\n",
      "Loss:  0.8675469160079956\n",
      "Loss:  0.8439159393310547\n",
      "Loss:  0.7486879825592041\n",
      "Loss:  0.7953909635543823\n",
      "Loss:  0.8212476968765259\n",
      "Loss:  0.7970259189605713\n",
      "Loss:  0.8582364320755005\n",
      "Loss:  0.7753674387931824\n",
      "Loss:  0.7865272760391235\n",
      "Loss:  0.9725350737571716\n",
      "Loss:  0.9482496976852417\n",
      "Loss:  0.9147405028343201\n",
      "Loss:  0.7954323291778564\n",
      "Loss:  0.8051652908325195\n",
      "Loss:  0.8423741459846497\n",
      "Loss:  0.9999489188194275\n",
      "Loss:  0.8708869814872742\n",
      "Loss:  0.9557703733444214\n",
      "Loss:  0.8546185493469238\n",
      "Loss:  0.8701498508453369\n",
      "Loss:  1.1448910236358643\n",
      "Loss:  0.7906957864761353\n",
      "Loss:  0.8567243814468384\n",
      "Loss:  0.7022348046302795\n",
      "Loss:  1.0897877216339111\n",
      "Loss:  0.7699894905090332\n",
      "Loss:  0.9213372468948364\n",
      "Loss:  0.9718791246414185\n",
      "Loss:  0.761091411113739\n",
      "Loss:  0.7971352338790894\n",
      "Loss:  0.8680880665779114\n",
      "Loss:  0.8234344720840454\n",
      "Loss:  0.8213402628898621\n",
      "Loss:  0.7588298320770264\n",
      "Loss:  0.8915038108825684\n",
      "Loss:  0.8053474426269531\n",
      "Loss:  0.8040964007377625\n",
      "Loss:  0.859725296497345\n",
      "Loss:  0.971562385559082\n",
      "Loss:  0.8549572825431824\n",
      "Loss:  0.7227364182472229\n",
      "Loss:  0.8634446263313293\n",
      "Loss:  0.9039921760559082\n",
      "Loss:  0.8033499717712402\n",
      "Loss:  0.749794602394104\n",
      "Loss:  0.775518000125885\n",
      "Loss:  0.9932489395141602\n",
      "Loss:  0.8574022650718689\n",
      "Loss:  0.685625433921814\n",
      "Loss:  0.815449059009552\n",
      "Loss:  0.9686157703399658\n",
      "Loss:  0.6633721590042114\n",
      "Loss:  0.8970069289207458\n",
      "Loss:  0.910908043384552\n",
      "Loss:  0.8606809973716736\n",
      "Loss:  0.8321097493171692\n",
      "Loss:  0.7957934141159058\n",
      "Loss:  0.9146344661712646\n",
      "Loss:  0.8043174743652344\n",
      "Loss:  0.8933956623077393\n",
      "Loss:  0.7502339482307434\n",
      "Loss:  0.9513025879859924\n",
      "Loss:  0.8073703050613403\n",
      "Loss:  0.9114987254142761\n",
      "Loss:  0.984418511390686\n",
      "Loss:  0.7848614454269409\n",
      "Loss:  0.9160775542259216\n",
      "Loss:  0.714154839515686\n",
      "Loss:  0.8554410934448242\n",
      "Loss:  0.8233419060707092\n",
      "Loss:  0.7049564123153687\n",
      "Loss:  0.9585429430007935\n",
      "Loss:  0.9196749329566956\n",
      "Loss:  0.979624330997467\n",
      "Loss:  0.9035003185272217\n",
      "Loss:  0.8506279587745667\n",
      "Loss:  1.1168335676193237\n",
      "Testing data accuracy: 73%\n",
      "Train Epoch: 8 Loss: 0.839742\n",
      "Train Epoch: 8 Loss: 0.770000\n",
      "Train Epoch: 8 Loss: 0.771995\n",
      "Train Epoch: 8 Loss: 0.773253\n",
      "Train Epoch: 8 Loss: 0.853925\n",
      "Train Epoch: 8 Loss: 0.859904\n",
      "Train Epoch: 8 Loss: 0.698912\n",
      "Train Epoch: 8 Loss: 0.912253\n",
      "Train Epoch: 8 Loss: 0.918424\n",
      "Train Epoch: 8 Loss: 0.903663\n",
      "Train Epoch: 8 Loss: 0.821894\n",
      "Train Epoch: 8 Loss: 0.859194\n",
      "Train Epoch: 8 Loss: 0.849419\n",
      "Train Epoch: 8 Loss: 0.815927\n",
      "Train Epoch: 8 Loss: 0.865930\n",
      "Train Epoch: 8 Loss: 0.749209\n",
      "Train Epoch: 8 Loss: 0.786891\n",
      "Train Epoch: 8 Loss: 0.905693\n",
      "Train Epoch: 8 Loss: 0.829320\n",
      "Train Epoch: 8 Loss: 0.866612\n",
      "Train Epoch: 8 Loss: 0.730336\n",
      "Train Epoch: 8 Loss: 0.859396\n",
      "Train Epoch: 8 Loss: 0.968263\n",
      "Train Epoch: 8 Loss: 0.894745\n",
      "Train Epoch: 8 Loss: 0.795058\n",
      "Train Epoch: 8 Loss: 0.929037\n",
      "Train Epoch: 8 Loss: 0.831398\n",
      "Train Epoch: 8 Loss: 0.795727\n",
      "Train Epoch: 8 Loss: 0.922608\n",
      "Train Epoch: 8 Loss: 0.884377\n",
      "Train Epoch: 8 Loss: 0.885963\n",
      "Train Epoch: 8 Loss: 0.681130\n",
      "Epoch:  8\n",
      "Loss:  0.7494759559631348\n",
      "Loss:  0.8457400798797607\n",
      "Loss:  0.8597431778907776\n",
      "Loss:  0.8275736570358276\n",
      "Loss:  0.9654465913772583\n",
      "Loss:  0.6186783909797668\n",
      "Loss:  0.8563699722290039\n",
      "Loss:  0.975287914276123\n",
      "Loss:  0.9144306182861328\n",
      "Loss:  0.727674126625061\n",
      "Loss:  0.8034026026725769\n",
      "Loss:  0.7624061703681946\n",
      "Loss:  0.7652642130851746\n",
      "Loss:  0.8934246301651001\n",
      "Loss:  0.7845073938369751\n",
      "Loss:  0.8016409277915955\n",
      "Loss:  0.722377598285675\n",
      "Loss:  0.9514503479003906\n",
      "Loss:  0.8228275179862976\n",
      "Loss:  0.7087114453315735\n",
      "Loss:  0.7280282378196716\n",
      "Loss:  0.7541736364364624\n",
      "Loss:  0.7979084253311157\n",
      "Loss:  0.792415201663971\n",
      "Loss:  0.7555776834487915\n",
      "Loss:  0.7446529269218445\n",
      "Loss:  0.8110751509666443\n",
      "Loss:  0.7247522473335266\n",
      "Loss:  0.6250705122947693\n",
      "Loss:  0.8339681029319763\n",
      "Loss:  0.9312504529953003\n",
      "Loss:  0.735668957233429\n",
      "Loss:  0.922505259513855\n",
      "Loss:  0.9081634879112244\n",
      "Loss:  0.8392912745475769\n",
      "Loss:  0.8321640491485596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.6235313415527344\n",
      "Loss:  0.9558982849121094\n",
      "Loss:  0.8261184096336365\n",
      "Loss:  0.6905320882797241\n",
      "Loss:  0.9263976216316223\n",
      "Loss:  1.058290719985962\n",
      "Loss:  0.7469702363014221\n",
      "Loss:  0.833731472492218\n",
      "Loss:  0.7887392640113831\n",
      "Loss:  0.673613429069519\n",
      "Loss:  0.7600741982460022\n",
      "Loss:  0.7755407691001892\n",
      "Loss:  0.6899694204330444\n",
      "Loss:  0.8910011053085327\n",
      "Loss:  0.9317919611930847\n",
      "Loss:  0.9199662804603577\n",
      "Loss:  0.7174293994903564\n",
      "Loss:  0.7155146598815918\n",
      "Loss:  0.8300571441650391\n",
      "Loss:  0.9073231816291809\n",
      "Loss:  0.8930461406707764\n",
      "Loss:  0.8474515676498413\n",
      "Loss:  0.8414669632911682\n",
      "Loss:  0.7325295805931091\n",
      "Loss:  0.8622193336486816\n",
      "Loss:  0.9186911582946777\n",
      "Loss:  0.8631566762924194\n",
      "Loss:  0.83653324842453\n",
      "Loss:  0.6008003354072571\n",
      "Loss:  0.7740240097045898\n",
      "Loss:  0.9818359017372131\n",
      "Loss:  0.6498076915740967\n",
      "Loss:  0.7607444524765015\n",
      "Loss:  0.8308331966400146\n",
      "Loss:  0.8326109647750854\n",
      "Loss:  0.957394540309906\n",
      "Loss:  0.8963180780410767\n",
      "Loss:  0.7693755626678467\n",
      "Loss:  0.7479621767997742\n",
      "Loss:  0.73785799741745\n",
      "Loss:  0.8615251779556274\n",
      "Loss:  0.8315244913101196\n",
      "Loss:  0.7851046919822693\n",
      "Loss:  0.9303128719329834\n",
      "Loss:  0.86675626039505\n",
      "Loss:  0.7004076838493347\n",
      "Loss:  0.7910252213478088\n",
      "Loss:  0.7961466908454895\n",
      "Loss:  0.9036609530448914\n",
      "Loss:  0.8082959651947021\n",
      "Loss:  0.8472270965576172\n",
      "Loss:  0.7093133330345154\n",
      "Loss:  0.9051650762557983\n",
      "Loss:  0.7448556423187256\n",
      "Loss:  0.7215365767478943\n",
      "Loss:  0.7125242352485657\n",
      "Loss:  0.8239437341690063\n",
      "Loss:  0.9261420965194702\n",
      "Loss:  0.9817758202552795\n",
      "Loss:  0.7505500316619873\n",
      "Loss:  0.8912345170974731\n",
      "Loss:  0.7551592588424683\n",
      "Loss:  0.7254964113235474\n",
      "Loss:  0.725029468536377\n",
      "Loss:  0.8955685496330261\n",
      "Loss:  0.8654161095619202\n",
      "Loss:  0.7139892578125\n",
      "Loss:  0.8069955110549927\n",
      "Loss:  0.6799317598342896\n",
      "Loss:  0.9994344115257263\n",
      "Loss:  0.8332971334457397\n",
      "Loss:  0.6661103963851929\n",
      "Loss:  0.9674285054206848\n",
      "Loss:  0.8239809274673462\n",
      "Loss:  0.8534709215164185\n",
      "Loss:  0.7726213335990906\n",
      "Loss:  0.65699303150177\n",
      "Loss:  0.9401142597198486\n",
      "Loss:  0.6857438683509827\n",
      "Loss:  0.8111302852630615\n",
      "Loss:  0.8643900156021118\n",
      "Loss:  0.8403856158256531\n",
      "Loss:  0.663198709487915\n",
      "Loss:  0.9767107963562012\n",
      "Loss:  0.6040411591529846\n",
      "Loss:  0.8573607206344604\n",
      "Loss:  0.7789645791053772\n",
      "Loss:  0.6636727452278137\n",
      "Loss:  0.9463599920272827\n",
      "Loss:  0.8828015923500061\n",
      "Loss:  0.8219705820083618\n",
      "Loss:  0.8160339593887329\n",
      "Loss:  0.827415406703949\n",
      "Loss:  0.701823353767395\n",
      "Loss:  0.7774971127510071\n",
      "Loss:  0.729570746421814\n",
      "Loss:  0.9682304859161377\n",
      "Loss:  0.8179187774658203\n",
      "Loss:  0.7269554138183594\n",
      "Loss:  0.848402202129364\n",
      "Loss:  0.8519952893257141\n",
      "Loss:  0.8085871934890747\n",
      "Loss:  0.9394434690475464\n",
      "Loss:  0.9049203991889954\n",
      "Loss:  0.8272432684898376\n",
      "Loss:  0.8155638575553894\n",
      "Loss:  0.9234505891799927\n",
      "Loss:  0.7312769889831543\n",
      "Loss:  0.9051051139831543\n",
      "Loss:  0.8824246525764465\n",
      "Loss:  0.758263111114502\n",
      "Loss:  0.9474470019340515\n",
      "Loss:  0.940454363822937\n",
      "Loss:  0.8026168346405029\n",
      "Loss:  0.7350040674209595\n",
      "Loss:  0.7176399827003479\n",
      "Loss:  0.763211727142334\n",
      "Loss:  0.7746623754501343\n",
      "Loss:  0.8247240781784058\n",
      "Loss:  0.7599102258682251\n",
      "Loss:  0.6912693977355957\n",
      "Testing data accuracy: 73%\n",
      "Train Epoch: 9 Loss: 0.722391\n",
      "Train Epoch: 9 Loss: 0.686779\n",
      "Train Epoch: 9 Loss: 0.838103\n",
      "Train Epoch: 9 Loss: 0.818885\n",
      "Train Epoch: 9 Loss: 0.798648\n",
      "Train Epoch: 9 Loss: 0.816501\n",
      "Train Epoch: 9 Loss: 0.877025\n",
      "Train Epoch: 9 Loss: 0.887481\n",
      "Train Epoch: 9 Loss: 0.617144\n",
      "Train Epoch: 9 Loss: 0.933132\n",
      "Train Epoch: 9 Loss: 0.859626\n",
      "Train Epoch: 9 Loss: 0.742518\n",
      "Train Epoch: 9 Loss: 0.747390\n",
      "Train Epoch: 9 Loss: 0.805092\n",
      "Train Epoch: 9 Loss: 0.744636\n",
      "Train Epoch: 9 Loss: 0.964585\n",
      "Train Epoch: 9 Loss: 0.890768\n",
      "Train Epoch: 9 Loss: 0.759382\n",
      "Train Epoch: 9 Loss: 0.892608\n",
      "Train Epoch: 9 Loss: 0.861804\n",
      "Train Epoch: 9 Loss: 0.898586\n",
      "Train Epoch: 9 Loss: 0.717173\n",
      "Train Epoch: 9 Loss: 0.658364\n",
      "Train Epoch: 9 Loss: 0.779451\n",
      "Train Epoch: 9 Loss: 0.847465\n",
      "Train Epoch: 9 Loss: 0.651714\n",
      "Train Epoch: 9 Loss: 0.679498\n",
      "Train Epoch: 9 Loss: 0.767023\n",
      "Train Epoch: 9 Loss: 0.757855\n",
      "Train Epoch: 9 Loss: 0.840312\n",
      "Train Epoch: 9 Loss: 0.677413\n",
      "Train Epoch: 9 Loss: 0.591565\n",
      "Epoch:  9\n",
      "Loss:  0.752816379070282\n",
      "Loss:  0.65203458070755\n",
      "Loss:  0.6233870387077332\n",
      "Loss:  0.7849565744400024\n",
      "Loss:  0.7748573422431946\n",
      "Loss:  0.7699117660522461\n",
      "Loss:  0.7970807552337646\n",
      "Loss:  0.6739450097084045\n",
      "Loss:  0.67037433385849\n",
      "Loss:  0.8608866930007935\n",
      "Loss:  0.8472078442573547\n",
      "Loss:  0.710681676864624\n",
      "Loss:  0.7833848595619202\n",
      "Loss:  0.6538341045379639\n",
      "Loss:  0.9174573421478271\n",
      "Loss:  0.8247909545898438\n",
      "Loss:  0.6926931142807007\n",
      "Loss:  0.8001148104667664\n",
      "Loss:  0.6237719058990479\n",
      "Loss:  0.6659749150276184\n",
      "Loss:  0.7787973284721375\n",
      "Loss:  0.7201797962188721\n",
      "Loss:  0.655730128288269\n",
      "Loss:  0.740879237651825\n",
      "Loss:  0.7084452509880066\n",
      "Loss:  0.771655797958374\n",
      "Loss:  0.6950408220291138\n",
      "Loss:  0.7463324666023254\n",
      "Loss:  0.879130482673645\n",
      "Loss:  0.7733513116836548\n",
      "Loss:  0.7493586540222168\n",
      "Loss:  0.8228527307510376\n",
      "Loss:  0.7408608794212341\n",
      "Loss:  0.7534855604171753\n",
      "Loss:  0.7042150497436523\n",
      "Loss:  0.7438519597053528\n",
      "Loss:  0.7121433615684509\n",
      "Loss:  0.5848501324653625\n",
      "Loss:  0.7777641415596008\n",
      "Loss:  0.8394565582275391\n",
      "Loss:  0.7677052021026611\n",
      "Loss:  0.7388302683830261\n",
      "Loss:  0.8633009791374207\n",
      "Loss:  0.8676778078079224\n",
      "Loss:  0.7822005748748779\n",
      "Loss:  0.735112726688385\n",
      "Loss:  0.7499560713768005\n",
      "Loss:  0.7327133417129517\n",
      "Loss:  0.9023842215538025\n",
      "Loss:  0.7282670140266418\n",
      "Loss:  0.6493980288505554\n",
      "Loss:  0.8015121221542358\n",
      "Loss:  0.6313637495040894\n",
      "Loss:  0.9144913554191589\n",
      "Loss:  0.8049286007881165\n",
      "Loss:  0.9934430122375488\n",
      "Loss:  0.838986337184906\n",
      "Loss:  0.643904447555542\n",
      "Loss:  0.8099864721298218\n",
      "Loss:  0.9906972646713257\n",
      "Loss:  0.7876185774803162\n",
      "Loss:  0.7857367396354675\n",
      "Loss:  0.7388138175010681\n",
      "Loss:  0.7626237869262695\n",
      "Loss:  0.7947879433631897\n",
      "Loss:  0.6858533620834351\n",
      "Loss:  0.7672939300537109\n",
      "Loss:  0.9035314321517944\n",
      "Loss:  0.900089681148529\n",
      "Loss:  0.7515586018562317\n",
      "Loss:  0.8928255438804626\n",
      "Loss:  0.7151014804840088\n",
      "Loss:  0.676872730255127\n",
      "Loss:  0.8356807827949524\n",
      "Loss:  0.6069296598434448\n",
      "Loss:  0.7129619121551514\n",
      "Loss:  0.5865581035614014\n",
      "Loss:  0.9310027956962585\n",
      "Loss:  0.7215203642845154\n",
      "Loss:  0.8743575811386108\n",
      "Loss:  0.8385103344917297\n",
      "Loss:  0.8359774947166443\n",
      "Loss:  0.9297526478767395\n",
      "Loss:  0.9296039938926697\n",
      "Loss:  0.7656732797622681\n",
      "Loss:  0.7333166599273682\n",
      "Loss:  0.8737236261367798\n",
      "Loss:  0.7409289479255676\n",
      "Loss:  0.7185048460960388\n",
      "Loss:  0.7788751125335693\n",
      "Loss:  0.8591928482055664\n",
      "Loss:  0.7911843061447144\n",
      "Loss:  0.5864953398704529\n",
      "Loss:  0.9237544536590576\n",
      "Loss:  0.6767655611038208\n",
      "Loss:  0.7281355261802673\n",
      "Loss:  0.8073096871376038\n",
      "Loss:  0.660189688205719\n",
      "Loss:  0.9114716649055481\n",
      "Loss:  0.7209249138832092\n",
      "Loss:  0.788825273513794\n",
      "Loss:  0.7731565237045288\n",
      "Loss:  0.8594875335693359\n",
      "Loss:  0.7769750952720642\n",
      "Loss:  0.7251458168029785\n",
      "Loss:  0.8767445087432861\n",
      "Loss:  0.7549619078636169\n",
      "Loss:  0.9226318001747131\n",
      "Loss:  0.8811064958572388\n",
      "Loss:  0.9413983225822449\n",
      "Loss:  0.8430410027503967\n",
      "Loss:  0.7905750274658203\n",
      "Loss:  0.8249943256378174\n",
      "Loss:  0.9714660048484802\n",
      "Loss:  0.7998889684677124\n",
      "Loss:  0.8446218371391296\n",
      "Loss:  0.9322350025177002\n",
      "Loss:  0.6900975704193115\n",
      "Loss:  0.7252945899963379\n",
      "Loss:  0.865314245223999\n",
      "Loss:  0.6851664185523987\n",
      "Loss:  0.8436356782913208\n",
      "Loss:  0.7853189706802368\n",
      "Loss:  0.674706757068634\n",
      "Loss:  0.9544011354446411\n",
      "Loss:  0.8932227492332458\n",
      "Loss:  0.7314043045043945\n",
      "Loss:  0.6966975927352905\n",
      "Loss:  0.881750762462616\n",
      "Loss:  0.6849415302276611\n",
      "Loss:  0.7179823517799377\n",
      "Loss:  0.790511965751648\n",
      "Loss:  0.749859094619751\n",
      "Loss:  0.6982354521751404\n",
      "Loss:  0.7621901631355286\n",
      "Loss:  0.7400767803192139\n",
      "Loss:  0.7845185995101929\n",
      "Loss:  0.7046021223068237\n",
      "Loss:  0.6586716175079346\n",
      "Loss:  0.6940006613731384\n",
      "Loss:  0.8300488591194153\n",
      "Loss:  0.6397280097007751\n",
      "Loss:  0.8766670823097229\n",
      "Loss:  0.8046041131019592\n",
      "Loss:  0.7776991128921509\n",
      "Loss:  0.7095279693603516\n",
      "Loss:  0.902229905128479\n",
      "Loss:  0.7282920479774475\n",
      "Loss:  0.6771706342697144\n",
      "Loss:  0.8968695402145386\n",
      "Loss:  0.7595648169517517\n",
      "Loss:  0.7214094400405884\n",
      "Loss:  0.7994797229766846\n",
      "Loss:  0.80027836561203\n",
      "Loss:  0.8435759544372559\n",
      "Loss:  0.7333704829216003\n",
      "Loss:  0.6147636771202087\n",
      "Testing data accuracy: 74%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(federated_train_loader):\n",
    "        data = data.view(data.shape[0], -1)\n",
    "        model.send(data.location)\n",
    "        output = model(data)\n",
    "        data, target = data.to('cpu'), target.to('cpu')\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.get()\n",
    "        if batch_idx % 30 == 0:\n",
    "            loss = loss.get() \n",
    "            print('Train Epoch: {} Loss: {:.6f}'.format(\n",
    "                epoch, loss.item()))\n",
    "    model.eval()\n",
    "    print('Epoch: ', epoch)\n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (data, target) in enumerate(testloader):\n",
    "        data = data.view(data.shape[0], -1)\n",
    "        output = model(data)\n",
    "        \n",
    "        loss = criterion(output, target)\n",
    "        print('Loss: ', loss.item())\n",
    "        max_arg_output = torch.argmax(output, dim=1)\n",
    "        total_correct += int(torch.sum(max_arg_output == target))\n",
    "        total += data.shape[0]\n",
    "    print('Testing data accuracy: {:.0%}'.format(total_correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
