{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets,transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0724 00:44:46.728399 140619060643584 secure_random.py:22] Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow (1.14.0). Fix this by compiling custom ops.\n",
      "W0724 00:44:46.833355 140619060643584 deprecation_wrapper.py:119] From /home/venktesh/anaconda3/envs/pysyft/lib/python3.6/site-packages/tf_encrypted/session.py:28: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import syft as sy\n",
    "import copy\n",
    "\n",
    "hook = sy.TorchHook(torch)\n",
    "from torch import nn,optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import helper\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize([0.5], [0.5])])\n",
    "# Download and load the training data\n",
    "trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "image, label = next(iter(trainloader))\n",
    "helper.imshow(image[0,:]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser:\n",
    "    \"\"\"Parameters for training\"\"\"\n",
    "    def __init__(self):\n",
    "        self.epochs = 10\n",
    "        self.lr = 0.001\n",
    "        self.test_batch_size = 8\n",
    "        self.batch_size = 8\n",
    "        self.log_interval = 10\n",
    "        self.seed = 1\n",
    "    \n",
    "args = Parser()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "kwargs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0724 00:44:51.057912 140619060643584 hook.py:97] Torch was already hooked... skipping hooking process\n"
     ]
    }
   ],
   "source": [
    "import syft as sy\n",
    "hook = sy.TorchHook(torch)\n",
    "bob = sy.VirtualWorker(hook, id=\"bob\")\n",
    "alice = sy.VirtualWorker(hook, id=\"alice\")\n",
    "venky = sy.VirtualWorker(hook, id=\"venky\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import logging\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "def dataset_federate(dataset, workers):\n",
    "    logger.info(\"Scanning and sending data to {}...\".format(\", \".join([w.id for w in workers])))\n",
    "\n",
    "    data_size = math.ceil(len(dataset) / len(workers))\n",
    "\n",
    "    datasets = []\n",
    "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=data_size, drop_last=True)\n",
    "    for dataset_idx, (data, targets) in enumerate(data_loader):\n",
    "        worker = workers[dataset_idx % len(workers)]\n",
    "        logger.debug(\"Sending data to worker %s\", worker.id)\n",
    "        data = data.send(worker)\n",
    "        targets = targets.send(worker)\n",
    "        datasets.append(sy.BaseDataset(data, targets))  # .send(worker)\n",
    "\n",
    "    logger.debug(\"Done!\")\n",
    "    return sy.FederatedDataset(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "federated_train_loader = sy.FederatedDataLoader(dataset_federate(trainset,(bob, alice)),\n",
    "                                                batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch import nn \n",
    "model = nn.Sequential(\n",
    "                    nn.Linear(784,128),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(128,64),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(64,10),\n",
    "                    nn.LogSoftmax(dim=1)\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import  optim \n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data,target) in enumerate(federated_train_loader):\n",
    "        data = data.view(data.shape[0], -1)\n",
    "        model.send(data.location)\n",
    "        output = model(data)\n",
    "        data, target = data.to('cpu'), target.to('cpu')\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.get()\n",
    "        if batch_idx % 30 == 0:\n",
    "            loss = loss.get() \n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * data.shape[0], len(federated_train_loader),\n",
    "                       100. * batch_idx / len(federated_train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (data, target) in enumerate(testloader):\n",
    "        data = data.view(data.shape[0], -1)\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        print('Loss: ', loss.item())\n",
    "        max_arg_output = torch.argmax(output, dim=1)\n",
    "        total_correct += int(torch.sum(max_arg_output == target))\n",
    "        total += data.shape[0]\n",
    "    print('Testing data accuracy: {:.0%}'.format(total_correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/938 (0%)]\tLoss: 2.321297\n",
      "Train Epoch: 1 [1920/938 (3%)]\tLoss: 2.318960\n",
      "Train Epoch: 1 [3840/938 (6%)]\tLoss: 2.311694\n",
      "Train Epoch: 1 [5760/938 (10%)]\tLoss: 2.299174\n",
      "Train Epoch: 1 [7680/938 (13%)]\tLoss: 2.283633\n",
      "Train Epoch: 1 [9600/938 (16%)]\tLoss: 2.289859\n",
      "Train Epoch: 1 [11520/938 (19%)]\tLoss: 2.293461\n",
      "Train Epoch: 1 [13440/938 (22%)]\tLoss: 2.299362\n",
      "Train Epoch: 1 [15360/938 (26%)]\tLoss: 2.289952\n",
      "Train Epoch: 1 [17280/938 (29%)]\tLoss: 2.290854\n",
      "Train Epoch: 1 [19200/938 (32%)]\tLoss: 2.281594\n",
      "Train Epoch: 1 [21120/938 (35%)]\tLoss: 2.286107\n",
      "Train Epoch: 1 [23040/938 (38%)]\tLoss: 2.261659\n",
      "Train Epoch: 1 [24960/938 (42%)]\tLoss: 2.254503\n",
      "Train Epoch: 1 [26880/938 (45%)]\tLoss: 2.244895\n",
      "Train Epoch: 1 [28800/938 (48%)]\tLoss: 2.272809\n",
      "Train Epoch: 1 [30720/938 (51%)]\tLoss: 2.216710\n",
      "Train Epoch: 1 [32640/938 (54%)]\tLoss: 2.250171\n",
      "Train Epoch: 1 [34560/938 (58%)]\tLoss: 2.226792\n",
      "Train Epoch: 1 [36480/938 (61%)]\tLoss: 2.251578\n",
      "Train Epoch: 1 [38400/938 (64%)]\tLoss: 2.254680\n",
      "Train Epoch: 1 [40320/938 (67%)]\tLoss: 2.238287\n",
      "Train Epoch: 1 [42240/938 (70%)]\tLoss: 2.235708\n",
      "Train Epoch: 1 [44160/938 (74%)]\tLoss: 2.222255\n",
      "Train Epoch: 1 [46080/938 (77%)]\tLoss: 2.225914\n",
      "Train Epoch: 1 [48000/938 (80%)]\tLoss: 2.227794\n",
      "Train Epoch: 1 [49920/938 (83%)]\tLoss: 2.200988\n",
      "Train Epoch: 1 [51840/938 (86%)]\tLoss: 2.267703\n",
      "Train Epoch: 1 [53760/938 (90%)]\tLoss: 2.212731\n",
      "Train Epoch: 1 [55680/938 (93%)]\tLoss: 2.226685\n",
      "Train Epoch: 1 [57600/938 (96%)]\tLoss: 2.170655\n",
      "Train Epoch: 1 [59520/938 (99%)]\tLoss: 2.187997\n",
      "Train Epoch: 2 [0/938 (0%)]\tLoss: 2.224565\n",
      "Train Epoch: 2 [1920/938 (3%)]\tLoss: 2.202605\n",
      "Train Epoch: 2 [3840/938 (6%)]\tLoss: 2.138221\n",
      "Train Epoch: 2 [5760/938 (10%)]\tLoss: 2.186883\n",
      "Train Epoch: 2 [7680/938 (13%)]\tLoss: 2.150864\n",
      "Train Epoch: 2 [9600/938 (16%)]\tLoss: 2.174724\n",
      "Train Epoch: 2 [11520/938 (19%)]\tLoss: 2.150437\n",
      "Train Epoch: 2 [13440/938 (22%)]\tLoss: 2.165898\n",
      "Train Epoch: 2 [15360/938 (26%)]\tLoss: 2.138138\n",
      "Train Epoch: 2 [17280/938 (29%)]\tLoss: 2.137445\n",
      "Train Epoch: 2 [19200/938 (32%)]\tLoss: 2.112493\n",
      "Train Epoch: 2 [21120/938 (35%)]\tLoss: 2.145442\n",
      "Train Epoch: 2 [23040/938 (38%)]\tLoss: 2.162285\n",
      "Train Epoch: 2 [24960/938 (42%)]\tLoss: 2.091935\n",
      "Train Epoch: 2 [26880/938 (45%)]\tLoss: 2.108490\n",
      "Train Epoch: 2 [28800/938 (48%)]\tLoss: 2.079418\n",
      "Train Epoch: 2 [30720/938 (51%)]\tLoss: 2.088930\n",
      "Train Epoch: 2 [32640/938 (54%)]\tLoss: 2.096956\n",
      "Train Epoch: 2 [34560/938 (58%)]\tLoss: 2.109499\n",
      "Train Epoch: 2 [36480/938 (61%)]\tLoss: 2.127377\n",
      "Train Epoch: 2 [38400/938 (64%)]\tLoss: 2.034016\n",
      "Train Epoch: 2 [40320/938 (67%)]\tLoss: 2.104335\n",
      "Train Epoch: 2 [42240/938 (70%)]\tLoss: 2.044428\n",
      "Train Epoch: 2 [44160/938 (74%)]\tLoss: 2.048830\n",
      "Train Epoch: 2 [46080/938 (77%)]\tLoss: 2.027926\n",
      "Train Epoch: 2 [48000/938 (80%)]\tLoss: 2.050439\n",
      "Train Epoch: 2 [49920/938 (83%)]\tLoss: 2.035602\n",
      "Train Epoch: 2 [51840/938 (86%)]\tLoss: 2.035608\n",
      "Train Epoch: 2 [53760/938 (90%)]\tLoss: 1.985363\n",
      "Train Epoch: 2 [55680/938 (93%)]\tLoss: 2.015881\n",
      "Train Epoch: 2 [57600/938 (96%)]\tLoss: 1.991062\n",
      "Train Epoch: 2 [59520/938 (99%)]\tLoss: 1.976767\n",
      "Train Epoch: 3 [0/938 (0%)]\tLoss: 1.964167\n",
      "Train Epoch: 3 [1920/938 (3%)]\tLoss: 1.967710\n",
      "Train Epoch: 3 [3840/938 (6%)]\tLoss: 1.938209\n",
      "Train Epoch: 3 [5760/938 (10%)]\tLoss: 1.983234\n",
      "Train Epoch: 3 [7680/938 (13%)]\tLoss: 1.970351\n",
      "Train Epoch: 3 [9600/938 (16%)]\tLoss: 1.922255\n",
      "Train Epoch: 3 [11520/938 (19%)]\tLoss: 1.934686\n",
      "Train Epoch: 3 [13440/938 (22%)]\tLoss: 1.916045\n",
      "Train Epoch: 3 [15360/938 (26%)]\tLoss: 1.866509\n",
      "Train Epoch: 3 [17280/938 (29%)]\tLoss: 1.829556\n",
      "Train Epoch: 3 [19200/938 (32%)]\tLoss: 1.905827\n",
      "Train Epoch: 3 [21120/938 (35%)]\tLoss: 1.894061\n",
      "Train Epoch: 3 [23040/938 (38%)]\tLoss: 1.860977\n",
      "Train Epoch: 3 [24960/938 (42%)]\tLoss: 1.842802\n",
      "Train Epoch: 3 [26880/938 (45%)]\tLoss: 1.854611\n",
      "Train Epoch: 3 [28800/938 (48%)]\tLoss: 1.776725\n",
      "Train Epoch: 3 [30720/938 (51%)]\tLoss: 1.900415\n",
      "Train Epoch: 3 [32640/938 (54%)]\tLoss: 1.824907\n",
      "Train Epoch: 3 [34560/938 (58%)]\tLoss: 1.857069\n",
      "Train Epoch: 3 [36480/938 (61%)]\tLoss: 1.722147\n",
      "Train Epoch: 3 [38400/938 (64%)]\tLoss: 1.779968\n",
      "Train Epoch: 3 [40320/938 (67%)]\tLoss: 1.785101\n",
      "Train Epoch: 3 [42240/938 (70%)]\tLoss: 1.730095\n",
      "Train Epoch: 3 [44160/938 (74%)]\tLoss: 1.757318\n",
      "Train Epoch: 3 [46080/938 (77%)]\tLoss: 1.759563\n",
      "Train Epoch: 3 [48000/938 (80%)]\tLoss: 1.786599\n",
      "Train Epoch: 3 [49920/938 (83%)]\tLoss: 1.732118\n",
      "Train Epoch: 3 [51840/938 (86%)]\tLoss: 1.864770\n",
      "Train Epoch: 3 [53760/938 (90%)]\tLoss: 1.773305\n",
      "Train Epoch: 3 [55680/938 (93%)]\tLoss: 1.693731\n",
      "Train Epoch: 3 [57600/938 (96%)]\tLoss: 1.768515\n",
      "Train Epoch: 3 [59520/938 (99%)]\tLoss: 1.704516\n",
      "Train Epoch: 4 [0/938 (0%)]\tLoss: 1.714389\n",
      "Train Epoch: 4 [1920/938 (3%)]\tLoss: 1.658534\n",
      "Train Epoch: 4 [3840/938 (6%)]\tLoss: 1.786073\n",
      "Train Epoch: 4 [5760/938 (10%)]\tLoss: 1.633722\n",
      "Train Epoch: 4 [7680/938 (13%)]\tLoss: 1.761249\n",
      "Train Epoch: 4 [9600/938 (16%)]\tLoss: 1.705431\n",
      "Train Epoch: 4 [11520/938 (19%)]\tLoss: 1.671973\n",
      "Train Epoch: 4 [13440/938 (22%)]\tLoss: 1.677343\n",
      "Train Epoch: 4 [15360/938 (26%)]\tLoss: 1.599034\n",
      "Train Epoch: 4 [17280/938 (29%)]\tLoss: 1.627979\n",
      "Train Epoch: 4 [19200/938 (32%)]\tLoss: 1.567722\n",
      "Train Epoch: 4 [21120/938 (35%)]\tLoss: 1.554002\n",
      "Train Epoch: 4 [23040/938 (38%)]\tLoss: 1.449360\n",
      "Train Epoch: 4 [24960/938 (42%)]\tLoss: 1.515373\n",
      "Train Epoch: 4 [26880/938 (45%)]\tLoss: 1.618996\n",
      "Train Epoch: 4 [28800/938 (48%)]\tLoss: 1.444820\n",
      "Train Epoch: 4 [30720/938 (51%)]\tLoss: 1.604434\n",
      "Train Epoch: 4 [32640/938 (54%)]\tLoss: 1.518938\n",
      "Train Epoch: 4 [34560/938 (58%)]\tLoss: 1.492072\n",
      "Train Epoch: 4 [36480/938 (61%)]\tLoss: 1.521051\n",
      "Train Epoch: 4 [38400/938 (64%)]\tLoss: 1.428804\n",
      "Train Epoch: 4 [40320/938 (67%)]\tLoss: 1.437438\n",
      "Train Epoch: 4 [42240/938 (70%)]\tLoss: 1.398585\n",
      "Train Epoch: 4 [44160/938 (74%)]\tLoss: 1.442928\n",
      "Train Epoch: 4 [46080/938 (77%)]\tLoss: 1.430101\n",
      "Train Epoch: 4 [48000/938 (80%)]\tLoss: 1.433405\n",
      "Train Epoch: 4 [49920/938 (83%)]\tLoss: 1.391245\n",
      "Train Epoch: 4 [51840/938 (86%)]\tLoss: 1.421408\n",
      "Train Epoch: 4 [53760/938 (90%)]\tLoss: 1.501803\n",
      "Train Epoch: 4 [55680/938 (93%)]\tLoss: 1.415282\n",
      "Train Epoch: 4 [57600/938 (96%)]\tLoss: 1.413578\n",
      "Train Epoch: 4 [59520/938 (99%)]\tLoss: 1.374942\n",
      "Train Epoch: 5 [0/938 (0%)]\tLoss: 1.368686\n",
      "Train Epoch: 5 [1920/938 (3%)]\tLoss: 1.415465\n",
      "Train Epoch: 5 [3840/938 (6%)]\tLoss: 1.299178\n",
      "Train Epoch: 5 [5760/938 (10%)]\tLoss: 1.300868\n",
      "Train Epoch: 5 [7680/938 (13%)]\tLoss: 1.245672\n",
      "Train Epoch: 5 [9600/938 (16%)]\tLoss: 1.268156\n",
      "Train Epoch: 5 [11520/938 (19%)]\tLoss: 1.311023\n",
      "Train Epoch: 5 [13440/938 (22%)]\tLoss: 1.328808\n",
      "Train Epoch: 5 [15360/938 (26%)]\tLoss: 1.307582\n",
      "Train Epoch: 5 [17280/938 (29%)]\tLoss: 1.291369\n",
      "Train Epoch: 5 [19200/938 (32%)]\tLoss: 1.312153\n",
      "Train Epoch: 5 [21120/938 (35%)]\tLoss: 1.163431\n",
      "Train Epoch: 5 [23040/938 (38%)]\tLoss: 1.256696\n",
      "Train Epoch: 5 [24960/938 (42%)]\tLoss: 1.276125\n",
      "Train Epoch: 5 [26880/938 (45%)]\tLoss: 1.237152\n",
      "Train Epoch: 5 [28800/938 (48%)]\tLoss: 1.192078\n",
      "Train Epoch: 5 [30720/938 (51%)]\tLoss: 1.413211\n",
      "Train Epoch: 5 [32640/938 (54%)]\tLoss: 1.185260\n",
      "Train Epoch: 5 [34560/938 (58%)]\tLoss: 1.258922\n",
      "Train Epoch: 5 [36480/938 (61%)]\tLoss: 1.249916\n",
      "Train Epoch: 5 [38400/938 (64%)]\tLoss: 1.317222\n",
      "Train Epoch: 5 [40320/938 (67%)]\tLoss: 1.243304\n",
      "Train Epoch: 5 [42240/938 (70%)]\tLoss: 1.236217\n",
      "Train Epoch: 5 [44160/938 (74%)]\tLoss: 1.212052\n",
      "Train Epoch: 5 [46080/938 (77%)]\tLoss: 1.235198\n",
      "Train Epoch: 5 [48000/938 (80%)]\tLoss: 1.281414\n",
      "Train Epoch: 5 [49920/938 (83%)]\tLoss: 1.206698\n",
      "Train Epoch: 5 [51840/938 (86%)]\tLoss: 1.242344\n",
      "Train Epoch: 5 [53760/938 (90%)]\tLoss: 1.155736\n",
      "Train Epoch: 5 [55680/938 (93%)]\tLoss: 1.154690\n",
      "Train Epoch: 5 [57600/938 (96%)]\tLoss: 1.161230\n",
      "Train Epoch: 5 [59520/938 (99%)]\tLoss: 1.198280\n",
      "Train Epoch: 6 [0/938 (0%)]\tLoss: 1.028209\n",
      "Train Epoch: 6 [1920/938 (3%)]\tLoss: 1.148577\n",
      "Train Epoch: 6 [3840/938 (6%)]\tLoss: 1.153665\n",
      "Train Epoch: 6 [5760/938 (10%)]\tLoss: 1.216804\n",
      "Train Epoch: 6 [7680/938 (13%)]\tLoss: 1.094704\n",
      "Train Epoch: 6 [9600/938 (16%)]\tLoss: 1.105141\n",
      "Train Epoch: 6 [11520/938 (19%)]\tLoss: 1.111897\n",
      "Train Epoch: 6 [13440/938 (22%)]\tLoss: 1.242977\n",
      "Train Epoch: 6 [15360/938 (26%)]\tLoss: 1.220880\n",
      "Train Epoch: 6 [17280/938 (29%)]\tLoss: 1.119213\n",
      "Train Epoch: 6 [19200/938 (32%)]\tLoss: 1.243283\n",
      "Train Epoch: 6 [21120/938 (35%)]\tLoss: 1.094959\n",
      "Train Epoch: 6 [23040/938 (38%)]\tLoss: 1.110879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [24960/938 (42%)]\tLoss: 1.064726\n",
      "Train Epoch: 6 [26880/938 (45%)]\tLoss: 1.131177\n",
      "Train Epoch: 6 [28800/938 (48%)]\tLoss: 1.152392\n",
      "Train Epoch: 6 [30720/938 (51%)]\tLoss: 1.034584\n",
      "Train Epoch: 6 [32640/938 (54%)]\tLoss: 1.003178\n",
      "Train Epoch: 6 [34560/938 (58%)]\tLoss: 1.031578\n",
      "Train Epoch: 6 [36480/938 (61%)]\tLoss: 1.084588\n",
      "Train Epoch: 6 [38400/938 (64%)]\tLoss: 1.117570\n",
      "Train Epoch: 6 [40320/938 (67%)]\tLoss: 1.076132\n",
      "Train Epoch: 6 [42240/938 (70%)]\tLoss: 0.941975\n",
      "Train Epoch: 6 [44160/938 (74%)]\tLoss: 1.062297\n",
      "Train Epoch: 6 [46080/938 (77%)]\tLoss: 1.074647\n",
      "Train Epoch: 6 [48000/938 (80%)]\tLoss: 1.073480\n",
      "Train Epoch: 6 [49920/938 (83%)]\tLoss: 1.073195\n",
      "Train Epoch: 6 [51840/938 (86%)]\tLoss: 0.910039\n",
      "Train Epoch: 6 [53760/938 (90%)]\tLoss: 0.987237\n",
      "Train Epoch: 6 [55680/938 (93%)]\tLoss: 1.051220\n",
      "Train Epoch: 6 [57600/938 (96%)]\tLoss: 1.095453\n",
      "Train Epoch: 6 [59520/938 (99%)]\tLoss: 1.093346\n",
      "Train Epoch: 7 [0/938 (0%)]\tLoss: 1.062591\n",
      "Train Epoch: 7 [1920/938 (3%)]\tLoss: 1.020011\n",
      "Train Epoch: 7 [3840/938 (6%)]\tLoss: 1.024564\n",
      "Train Epoch: 7 [5760/938 (10%)]\tLoss: 0.955111\n",
      "Train Epoch: 7 [7680/938 (13%)]\tLoss: 1.159278\n",
      "Train Epoch: 7 [9600/938 (16%)]\tLoss: 1.010228\n",
      "Train Epoch: 7 [11520/938 (19%)]\tLoss: 0.973700\n",
      "Train Epoch: 7 [13440/938 (22%)]\tLoss: 1.167203\n",
      "Train Epoch: 7 [15360/938 (26%)]\tLoss: 0.998123\n",
      "Train Epoch: 7 [17280/938 (29%)]\tLoss: 1.060613\n",
      "Train Epoch: 7 [19200/938 (32%)]\tLoss: 1.162613\n",
      "Train Epoch: 7 [21120/938 (35%)]\tLoss: 1.030926\n",
      "Train Epoch: 7 [23040/938 (38%)]\tLoss: 0.968605\n",
      "Train Epoch: 7 [24960/938 (42%)]\tLoss: 1.089900\n",
      "Train Epoch: 7 [26880/938 (45%)]\tLoss: 1.063122\n",
      "Train Epoch: 7 [28800/938 (48%)]\tLoss: 1.079701\n",
      "Train Epoch: 7 [30720/938 (51%)]\tLoss: 1.032491\n",
      "Train Epoch: 7 [32640/938 (54%)]\tLoss: 0.988166\n",
      "Train Epoch: 7 [34560/938 (58%)]\tLoss: 0.886487\n",
      "Train Epoch: 7 [36480/938 (61%)]\tLoss: 0.935866\n",
      "Train Epoch: 7 [38400/938 (64%)]\tLoss: 1.012937\n",
      "Train Epoch: 7 [40320/938 (67%)]\tLoss: 0.995197\n",
      "Train Epoch: 7 [42240/938 (70%)]\tLoss: 0.995553\n",
      "Train Epoch: 7 [44160/938 (74%)]\tLoss: 0.941875\n",
      "Train Epoch: 7 [46080/938 (77%)]\tLoss: 0.842268\n",
      "Train Epoch: 7 [48000/938 (80%)]\tLoss: 0.839392\n",
      "Train Epoch: 7 [49920/938 (83%)]\tLoss: 0.777823\n",
      "Train Epoch: 7 [51840/938 (86%)]\tLoss: 0.804062\n",
      "Train Epoch: 7 [53760/938 (90%)]\tLoss: 0.961396\n",
      "Train Epoch: 7 [55680/938 (93%)]\tLoss: 0.971729\n",
      "Train Epoch: 7 [57600/938 (96%)]\tLoss: 1.001347\n",
      "Train Epoch: 7 [59520/938 (99%)]\tLoss: 0.941900\n",
      "Train Epoch: 8 [0/938 (0%)]\tLoss: 0.777290\n",
      "Train Epoch: 8 [1920/938 (3%)]\tLoss: 0.894307\n",
      "Train Epoch: 8 [3840/938 (6%)]\tLoss: 0.899109\n",
      "Train Epoch: 8 [5760/938 (10%)]\tLoss: 1.065393\n",
      "Train Epoch: 8 [7680/938 (13%)]\tLoss: 0.913196\n",
      "Train Epoch: 8 [9600/938 (16%)]\tLoss: 0.889431\n",
      "Train Epoch: 8 [11520/938 (19%)]\tLoss: 0.956576\n",
      "Train Epoch: 8 [13440/938 (22%)]\tLoss: 1.018532\n",
      "Train Epoch: 8 [15360/938 (26%)]\tLoss: 0.892507\n",
      "Train Epoch: 8 [17280/938 (29%)]\tLoss: 0.984262\n",
      "Train Epoch: 8 [19200/938 (32%)]\tLoss: 0.898849\n",
      "Train Epoch: 8 [21120/938 (35%)]\tLoss: 0.784213\n",
      "Train Epoch: 8 [23040/938 (38%)]\tLoss: 0.956237\n",
      "Train Epoch: 8 [24960/938 (42%)]\tLoss: 0.788317\n",
      "Train Epoch: 8 [26880/938 (45%)]\tLoss: 0.888527\n",
      "Train Epoch: 8 [28800/938 (48%)]\tLoss: 0.768129\n",
      "Train Epoch: 8 [30720/938 (51%)]\tLoss: 0.942116\n",
      "Train Epoch: 8 [32640/938 (54%)]\tLoss: 0.853906\n",
      "Train Epoch: 8 [34560/938 (58%)]\tLoss: 0.866051\n",
      "Train Epoch: 8 [36480/938 (61%)]\tLoss: 0.875608\n",
      "Train Epoch: 8 [38400/938 (64%)]\tLoss: 0.840553\n",
      "Train Epoch: 8 [40320/938 (67%)]\tLoss: 0.895486\n",
      "Train Epoch: 8 [42240/938 (70%)]\tLoss: 0.955198\n",
      "Train Epoch: 8 [44160/938 (74%)]\tLoss: 0.922992\n",
      "Train Epoch: 8 [46080/938 (77%)]\tLoss: 0.891714\n",
      "Train Epoch: 8 [48000/938 (80%)]\tLoss: 0.817738\n",
      "Train Epoch: 8 [49920/938 (83%)]\tLoss: 0.935709\n",
      "Train Epoch: 8 [51840/938 (86%)]\tLoss: 0.865379\n",
      "Train Epoch: 8 [53760/938 (90%)]\tLoss: 0.768692\n",
      "Train Epoch: 8 [55680/938 (93%)]\tLoss: 0.831774\n",
      "Train Epoch: 8 [57600/938 (96%)]\tLoss: 0.965674\n",
      "Train Epoch: 8 [59520/938 (99%)]\tLoss: 0.844703\n",
      "Train Epoch: 9 [0/938 (0%)]\tLoss: 0.926329\n",
      "Train Epoch: 9 [1920/938 (3%)]\tLoss: 1.053622\n",
      "Train Epoch: 9 [3840/938 (6%)]\tLoss: 0.777460\n",
      "Train Epoch: 9 [5760/938 (10%)]\tLoss: 0.834702\n",
      "Train Epoch: 9 [7680/938 (13%)]\tLoss: 0.753447\n",
      "Train Epoch: 9 [9600/938 (16%)]\tLoss: 0.902683\n",
      "Train Epoch: 9 [11520/938 (19%)]\tLoss: 0.846165\n",
      "Train Epoch: 9 [13440/938 (22%)]\tLoss: 0.829245\n",
      "Train Epoch: 9 [15360/938 (26%)]\tLoss: 0.928755\n",
      "Train Epoch: 9 [17280/938 (29%)]\tLoss: 0.909034\n",
      "Train Epoch: 9 [19200/938 (32%)]\tLoss: 0.774131\n",
      "Train Epoch: 9 [21120/938 (35%)]\tLoss: 0.809401\n",
      "Train Epoch: 9 [23040/938 (38%)]\tLoss: 0.811864\n",
      "Train Epoch: 9 [24960/938 (42%)]\tLoss: 0.859220\n",
      "Train Epoch: 9 [26880/938 (45%)]\tLoss: 0.795019\n",
      "Train Epoch: 9 [28800/938 (48%)]\tLoss: 0.884405\n",
      "Train Epoch: 9 [30720/938 (51%)]\tLoss: 0.850344\n",
      "Train Epoch: 9 [32640/938 (54%)]\tLoss: 0.892008\n",
      "Train Epoch: 9 [34560/938 (58%)]\tLoss: 0.851896\n",
      "Train Epoch: 9 [36480/938 (61%)]\tLoss: 0.735219\n",
      "Train Epoch: 9 [38400/938 (64%)]\tLoss: 0.770527\n",
      "Train Epoch: 9 [40320/938 (67%)]\tLoss: 1.011878\n",
      "Train Epoch: 9 [42240/938 (70%)]\tLoss: 0.994571\n",
      "Train Epoch: 9 [44160/938 (74%)]\tLoss: 0.740607\n",
      "Train Epoch: 9 [46080/938 (77%)]\tLoss: 0.676888\n",
      "Train Epoch: 9 [48000/938 (80%)]\tLoss: 0.673369\n",
      "Train Epoch: 9 [49920/938 (83%)]\tLoss: 0.784145\n",
      "Train Epoch: 9 [51840/938 (86%)]\tLoss: 0.732398\n",
      "Train Epoch: 9 [53760/938 (90%)]\tLoss: 0.825863\n",
      "Train Epoch: 9 [55680/938 (93%)]\tLoss: 0.729527\n",
      "Train Epoch: 9 [57600/938 (96%)]\tLoss: 0.846100\n",
      "Train Epoch: 9 [59520/938 (99%)]\tLoss: 0.849755\n",
      "Train Epoch: 10 [0/938 (0%)]\tLoss: 0.979512\n",
      "Train Epoch: 10 [1920/938 (3%)]\tLoss: 0.864890\n",
      "Train Epoch: 10 [3840/938 (6%)]\tLoss: 0.715252\n",
      "Train Epoch: 10 [5760/938 (10%)]\tLoss: 0.721356\n",
      "Train Epoch: 10 [7680/938 (13%)]\tLoss: 0.839123\n",
      "Train Epoch: 10 [9600/938 (16%)]\tLoss: 0.887787\n",
      "Train Epoch: 10 [11520/938 (19%)]\tLoss: 0.785097\n",
      "Train Epoch: 10 [13440/938 (22%)]\tLoss: 0.920973\n",
      "Train Epoch: 10 [15360/938 (26%)]\tLoss: 0.747092\n",
      "Train Epoch: 10 [17280/938 (29%)]\tLoss: 0.917418\n",
      "Train Epoch: 10 [19200/938 (32%)]\tLoss: 0.691844\n",
      "Train Epoch: 10 [21120/938 (35%)]\tLoss: 0.882216\n",
      "Train Epoch: 10 [23040/938 (38%)]\tLoss: 0.684234\n",
      "Train Epoch: 10 [24960/938 (42%)]\tLoss: 0.727350\n",
      "Train Epoch: 10 [26880/938 (45%)]\tLoss: 1.099354\n",
      "Train Epoch: 10 [28800/938 (48%)]\tLoss: 0.764503\n",
      "Train Epoch: 10 [30720/938 (51%)]\tLoss: 0.796150\n",
      "Train Epoch: 10 [32640/938 (54%)]\tLoss: 0.769629\n",
      "Train Epoch: 10 [34560/938 (58%)]\tLoss: 0.873810\n",
      "Train Epoch: 10 [36480/938 (61%)]\tLoss: 0.862999\n",
      "Train Epoch: 10 [38400/938 (64%)]\tLoss: 0.764549\n",
      "Train Epoch: 10 [40320/938 (67%)]\tLoss: 0.814748\n",
      "Train Epoch: 10 [42240/938 (70%)]\tLoss: 0.727362\n",
      "Train Epoch: 10 [44160/938 (74%)]\tLoss: 0.876784\n",
      "Train Epoch: 10 [46080/938 (77%)]\tLoss: 0.840209\n",
      "Train Epoch: 10 [48000/938 (80%)]\tLoss: 0.630252\n",
      "Train Epoch: 10 [49920/938 (83%)]\tLoss: 0.908963\n",
      "Train Epoch: 10 [51840/938 (86%)]\tLoss: 0.816322\n",
      "Train Epoch: 10 [53760/938 (90%)]\tLoss: 0.729491\n",
      "Train Epoch: 10 [55680/938 (93%)]\tLoss: 0.655487\n",
      "Train Epoch: 10 [57600/938 (96%)]\tLoss: 0.763233\n",
      "Train Epoch: 10 [59520/938 (99%)]\tLoss: 0.749501\n",
      "Total 1124.47 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t = time.time()\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(epoch)\n",
    "\n",
    "    \n",
    "total_time = time.time() - t\n",
    "print('Total', round(total_time, 2), 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.7813082933425903\n",
      "Loss:  0.7666496634483337\n",
      "Loss:  0.7202309370040894\n",
      "Loss:  0.7267113924026489\n",
      "Loss:  0.9076485633850098\n",
      "Loss:  0.8181501626968384\n",
      "Loss:  0.9944139719009399\n",
      "Loss:  0.6482468843460083\n",
      "Loss:  0.8166279792785645\n",
      "Loss:  0.8444867134094238\n",
      "Loss:  0.7569257616996765\n",
      "Loss:  0.6055858135223389\n",
      "Loss:  0.72182297706604\n",
      "Loss:  0.7176955938339233\n",
      "Loss:  0.8169050216674805\n",
      "Loss:  0.6702069640159607\n",
      "Loss:  0.7718925476074219\n",
      "Loss:  0.6673048734664917\n",
      "Loss:  0.6908633708953857\n",
      "Loss:  0.8281676173210144\n",
      "Loss:  0.8019407987594604\n",
      "Loss:  0.6937777996063232\n",
      "Loss:  0.7432273626327515\n",
      "Loss:  0.8164781332015991\n",
      "Loss:  0.7612770199775696\n",
      "Loss:  0.5884672999382019\n",
      "Loss:  0.9647849202156067\n",
      "Loss:  0.7932865619659424\n",
      "Loss:  0.7346975803375244\n",
      "Loss:  0.7852027416229248\n",
      "Loss:  0.8854793906211853\n",
      "Loss:  0.6858309507369995\n",
      "Loss:  0.7235745787620544\n",
      "Loss:  0.8781285881996155\n",
      "Loss:  0.8231803178787231\n",
      "Loss:  0.6949052810668945\n",
      "Loss:  0.9221354126930237\n",
      "Loss:  0.7609634399414062\n",
      "Loss:  0.8185581564903259\n",
      "Loss:  1.0186808109283447\n",
      "Loss:  0.7181522250175476\n",
      "Loss:  0.6819556355476379\n",
      "Loss:  0.7429521083831787\n",
      "Loss:  0.687847375869751\n",
      "Loss:  0.7093337178230286\n",
      "Loss:  0.6802054047584534\n",
      "Loss:  0.6448739767074585\n",
      "Loss:  0.6973791718482971\n",
      "Loss:  0.7669699788093567\n",
      "Loss:  0.7457194924354553\n",
      "Loss:  0.8094837069511414\n",
      "Loss:  0.912562906742096\n",
      "Loss:  0.8771551251411438\n",
      "Loss:  0.7028174996376038\n",
      "Loss:  0.8296902179718018\n",
      "Loss:  0.6958101987838745\n",
      "Loss:  0.8398771286010742\n",
      "Loss:  0.781474769115448\n",
      "Loss:  0.7403375506401062\n",
      "Loss:  0.6391121745109558\n",
      "Loss:  0.6282141804695129\n",
      "Loss:  0.7975876927375793\n",
      "Loss:  0.7395042181015015\n",
      "Loss:  0.766449511051178\n",
      "Loss:  0.8146318197250366\n",
      "Loss:  0.6330370903015137\n",
      "Loss:  0.6963955163955688\n",
      "Loss:  0.9088911414146423\n",
      "Loss:  0.7503885626792908\n",
      "Loss:  0.8082529306411743\n",
      "Loss:  0.8268478512763977\n",
      "Loss:  0.9180974960327148\n",
      "Loss:  0.6616752743721008\n",
      "Loss:  0.9171971082687378\n",
      "Loss:  0.7747274041175842\n",
      "Loss:  0.913910448551178\n",
      "Loss:  0.8481292128562927\n",
      "Loss:  0.7390532493591309\n",
      "Loss:  0.663794219493866\n",
      "Loss:  0.7697733640670776\n",
      "Loss:  1.0878710746765137\n",
      "Loss:  0.9198956489562988\n",
      "Loss:  0.7399550676345825\n",
      "Loss:  0.7795268297195435\n",
      "Loss:  0.8790565729141235\n",
      "Loss:  0.7172985672950745\n",
      "Loss:  0.7966030240058899\n",
      "Loss:  0.8959192037582397\n",
      "Loss:  0.7391151189804077\n",
      "Loss:  0.7329457998275757\n",
      "Loss:  0.6658548712730408\n",
      "Loss:  0.7072041034698486\n",
      "Loss:  0.5853081941604614\n",
      "Loss:  0.79243403673172\n",
      "Loss:  0.748397707939148\n",
      "Loss:  0.7487784028053284\n",
      "Loss:  0.9058239459991455\n",
      "Loss:  0.7136310935020447\n",
      "Loss:  0.9115022420883179\n",
      "Loss:  0.651518702507019\n",
      "Loss:  0.7302749752998352\n",
      "Loss:  0.7971141338348389\n",
      "Loss:  0.7892716526985168\n",
      "Loss:  0.8733975291252136\n",
      "Loss:  0.8014661073684692\n",
      "Loss:  0.8134556412696838\n",
      "Loss:  0.8303136825561523\n",
      "Loss:  0.7468940615653992\n",
      "Loss:  0.7040548324584961\n",
      "Loss:  0.7975454330444336\n",
      "Loss:  0.7022375464439392\n",
      "Loss:  0.7672922015190125\n",
      "Loss:  0.9781972765922546\n",
      "Loss:  0.7791394591331482\n",
      "Loss:  0.6649668216705322\n",
      "Loss:  0.8137926459312439\n",
      "Loss:  0.9148143529891968\n",
      "Loss:  0.7852286100387573\n",
      "Loss:  0.7113361358642578\n",
      "Loss:  0.7918738126754761\n",
      "Loss:  0.8112646341323853\n",
      "Loss:  0.8702890872955322\n",
      "Loss:  0.7694307565689087\n",
      "Loss:  0.742386519908905\n",
      "Loss:  0.6323679685592651\n",
      "Loss:  0.7713571190834045\n",
      "Loss:  0.7392542362213135\n",
      "Loss:  0.9005019664764404\n",
      "Loss:  0.679391622543335\n",
      "Loss:  0.6872783899307251\n",
      "Loss:  0.7982341051101685\n",
      "Loss:  0.8725002408027649\n",
      "Loss:  0.9035360813140869\n",
      "Loss:  0.7579750418663025\n",
      "Loss:  0.8273648619651794\n",
      "Loss:  0.7685580849647522\n",
      "Loss:  0.7736650109291077\n",
      "Loss:  0.6107664704322815\n",
      "Loss:  0.9385961890220642\n",
      "Loss:  0.8015574812889099\n",
      "Loss:  0.8102437257766724\n",
      "Loss:  0.8478871583938599\n",
      "Loss:  0.9361658692359924\n",
      "Loss:  0.9972535371780396\n",
      "Loss:  0.9583997130393982\n",
      "Loss:  0.8327975273132324\n",
      "Loss:  0.8602173924446106\n",
      "Loss:  0.8028455376625061\n",
      "Loss:  0.6742534041404724\n",
      "Loss:  0.7812293767929077\n",
      "Loss:  0.7163841724395752\n",
      "Loss:  0.6703556180000305\n",
      "Loss:  0.7973095774650574\n",
      "Loss:  0.6622202396392822\n",
      "Loss:  0.7280382513999939\n",
      "Loss:  0.7697690725326538\n",
      "Loss:  0.778516411781311\n",
      "Testing data accuracy: 74%\n"
     ]
    }
   ],
   "source": [
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding encryption. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First individual models for bob and alice are assigned  and  the datasets are reprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "remote_dataset = (list(),list())\n",
    "\n",
    "for batch_idx, (data,target) in enumerate(trainloader):\n",
    "    data = data.send([bob,alice][batch_idx % len([bob,alice])])\n",
    "    target = target.send([bob,alice][batch_idx % len([bob,alice])])\n",
    "    remote_dataset[batch_idx % len([bob,alice])].append((data, target))\n",
    "\n",
    "def update(data, target, model, optimizer):\n",
    "    data = data.view(data.shape[0], -1)\n",
    "    model.send(data.location)\n",
    "    optimizer.zero_grad()\n",
    "    pred = model(data)\n",
    "    loss = criterion(pred,target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return model\n",
    "def define_model():\n",
    "    return nn.Sequential(\n",
    "                    nn.Linear(784,128),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(128,64),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(64,10),\n",
    "                    nn.LogSoftmax(dim=1)\n",
    "                    )\n",
    "\n",
    "bobs_model = define_model()\n",
    "alices_model = define_model()\n",
    "\n",
    "bobs_optimizer = optim.SGD(bobs_model.parameters(), lr=args.lr)\n",
    "alices_optimizer = optim.SGD(alices_model.parameters(), lr=args.lr)\n",
    "\n",
    "models = [bobs_model, alices_model]\n",
    "params = [list(bobs_model.parameters()), list(alices_model.parameters())]\n",
    "optimizers = [bobs_optimizer, alices_optimizer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(epoch):\n",
    "    for data_index in range(len(remote_dataset[0])-1):\n",
    "        # update remote models\n",
    "        for remote_index in range(len([bob,alice])):\n",
    "            data, target = remote_dataset[remote_index][data_index]\n",
    "            models[remote_index] = update(data, target, models[remote_index], optimizers[remote_index])\n",
    "\n",
    "        # encrypted aggregation\n",
    "        new_params = list()\n",
    "        for param_i in range(len(params[0])):\n",
    "            shared_params = list()\n",
    "            for remote_index in range(len([bob,alice])):\n",
    "                shared_params.append(params[remote_index][param_i].copy().fix_precision().share(bob, alice, crypto_provider=venky).get())\n",
    "\n",
    "            new_param = (shared_params[0] + shared_params[1]).get().float_precision()/2\n",
    "            new_params.append(new_param)\n",
    "\n",
    "        # cleanup\n",
    "        with torch.no_grad():\n",
    "            for model in params:\n",
    "                for param in model:\n",
    "                    param *= 0\n",
    "\n",
    "            for model in models:\n",
    "                model.get()\n",
    "\n",
    "            for remote_index in range(len([bob,alice])):\n",
    "                for param_index in range(len(params[remote_index])):\n",
    "                    params[remote_index][param_index].set_(new_params[param_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test():\n",
    "    mode docls[0].eval()\n",
    "    test_loss = 0\n",
    "    for data, target in testloader:\n",
    "        data = data.view(data.shape[0], -1)\n",
    "        output = models[0](data)\n",
    "        test_loss += criterion(output, target).item() # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        \n",
    "    test_loss /= len(testloader.dataset)\n",
    "    print('Test set: Average loss: {:.4f}\\n'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Test set: Average loss: 0.0346\n",
      "\n",
      "Epoch 2\n",
      "Test set: Average loss: 0.0350\n",
      "\n",
      "Epoch 3\n",
      "Test set: Average loss: 0.0352\n",
      "\n",
      "Epoch 4\n",
      "Test set: Average loss: 0.0353\n",
      "\n",
      "Epoch 5\n",
      "Test set: Average loss: 0.0354\n",
      "\n",
      "Epoch 6\n",
      "Test set: Average loss: 0.0354\n",
      "\n",
      "Epoch 7\n",
      "Test set: Average loss: 0.0355\n",
      "\n",
      "Epoch 8\n",
      "Test set: Average loss: 0.0355\n",
      "\n",
      "Epoch 9\n",
      "Test set: Average loss: 0.0355\n",
      "\n",
      "Epoch 10\n",
      "Test set: Average loss: 0.0356\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    print(f\"Epoch {epoch + 1}\")\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
